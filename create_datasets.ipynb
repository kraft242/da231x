{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Datasets\n",
    "\n",
    "This notebook reads all SweLL-gold source files and converts them into transformers-compatible Datasets.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Ensure you have the SweLL-gold corpus downloaded and a `.env` file in the repository root with a path to the SweLL-gold directory, i.e:\n",
    "```\n",
    "SWELL_DIR=<PATH TO SWELL DIRECTORY>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv, path, makedirs\n",
    "from datasets import Dataset\n",
    "from prompts import minimal_prompt, fluency_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"LumiOpen/Viking-33B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions to Read Corpus Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SweLL-directory path from .env file\n",
    "load_dotenv()\n",
    "swell_dir = getenv(\"SWELL_DIR\")\n",
    "\n",
    "\n",
    "def get_file_paths(file_type):\n",
    "    \"\"\"\n",
    "    Gets the paths to the files containing dev, test, and train split for the given file_type.\n",
    "    The file_type must be one of: orig (source), ref1 (minimal edit), or ref2 (fluency edit)\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure file_type is correct\n",
    "    assert file_type in [\"orig\", \"ref1\", \"ref2\"]\n",
    "\n",
    "    base_name = f\"sv-swell_gold-{file_type}\"\n",
    "    splits = [\"dev\", \"test\", \"train\"]\n",
    "    return {split: path.join(swell_dir, f\"{base_name}-{split}.md\") for split in splits}\n",
    "\n",
    "\n",
    "def md_to_dict(md):\n",
    "    \"\"\"\n",
    "    From the multigec-2025-data-providers repo:\n",
    "    - https://github.com/spraakbanken/multigec-2025-data-providers\n",
    "\n",
    "    Parse shared task format into a dictionary where keys are essay IDs\n",
    "    and values are essay texts.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    md --- a string with the content of a shared task Markdown file.\n",
    "    \"\"\"\n",
    "    essay_dict = {}\n",
    "    for essay in md.split(\"### essay_id = \")[1:]:\n",
    "        (essay_id, text) = essay.split(\"\\n\", maxsplit=1)\n",
    "        essay_dict[essay_id] = text.strip(\"\\n\")\n",
    "    return essay_dict\n",
    "\n",
    "\n",
    "def read_file_to_dict(fp):\n",
    "    \"\"\"\n",
    "    Reads the file path into a dictionary, by using the function md_to_dict.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    fp --- The File path to read.\n",
    "    \"\"\"\n",
    "    with open(fp) as f:\n",
    "        md = f.read()\n",
    "    return md_to_dict(md)\n",
    "\n",
    "\n",
    "def read_files_to_dicts(file_type):\n",
    "    \"\"\"\n",
    "    Reads the corresponding files for file_type (orig, ref1, ref2) into a dictionary with keys dev, test, and train and values {essay_id: essay_text}.\n",
    "    \"\"\"\n",
    "    file_paths = get_file_paths(file_type)\n",
    "    return {\n",
    "        split: read_file_to_dict(file_path) for split, file_path in file_paths.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files into Dictionaries\n",
    "\n",
    "Each file is read into a dictionary on the format:\n",
    "\n",
    "```python\n",
    "{split: {essay_id: essay_text}}\n",
    "```\n",
    "\n",
    "Where split is in [\"dev\", \"train\", \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text_dicts = read_files_to_dicts(\"orig\")\n",
    "minimal_text_dicts = read_files_to_dicts(\"ref1\")\n",
    "fluency_text_dicts = read_files_to_dicts(\"ref2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset-Creation Helper Functions\n",
    "\n",
    "These helper functions are used to create the Hugging-Face datasets Dataset dictionaries that are used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_source_target_dict(src_dict, dst_dict, prompt):\n",
    "    \"\"\"\n",
    "    Create Hugging-Face transformers-compatible source-target dicts.\n",
    "    First ensures both dicts have equal keys for a correct mapping.\n",
    "    Then sorts both dicts on essay_id keys and inserts the essay_texts, in the same order, into a dict with the format:\n",
    "\n",
    "    ```python\n",
    "    {\n",
    "        \"source\": [prompt + text]\n",
    "        \"target\": [text]\n",
    "    }\n",
    "    ```\n",
    "    Where the prompt is either the minimal-edit prompt or the fluency-edit prompt.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    src_dict --- A dict {essay_id: essay_text} containing the source (uncorrected) texts.\n",
    "    dst_dict --- A dict {essay_id: essay_text} containg the target (corrected) texts.\n",
    "    prompt --- The prompt to insert before each source text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure both input dictionaries have equal keys\n",
    "    assert src_dict.keys() == dst_dict.keys()\n",
    "\n",
    "    # Ensure both lists have same order\n",
    "    sorted_src = sorted(src_dict.items())\n",
    "    sorted_dst = sorted(dst_dict.items())\n",
    "\n",
    "    return {\n",
    "        \"source\": [prompt + text for _, text in sorted_src],\n",
    "        \"target\": [text for _, text in sorted_dst],\n",
    "    }\n",
    "\n",
    "\n",
    "def create_dataset_dict(src_dict, dst_dict, split, prompt):\n",
    "    \"\"\"\n",
    "    Creates a Hugging-Face Dataset dict with the same split from the src_dict and dst_dict.\n",
    "\n",
    "    Arguments:\n",
    "    src_dict --- A dict {essay_id: essay_text} containing the source (uncorrected) texts.\n",
    "    dst_dict --- A dict {essay_id: essay_text} containg the target (corrected) texts.\n",
    "    split --- The split to create the dict from.\n",
    "    prompt --- The prompt to insert before each source text.\n",
    "    \"\"\"\n",
    "    return Dataset.from_dict(\n",
    "        create_source_target_dict(src_dict[split], dst_dict[split], prompt)\n",
    "    )\n",
    "\n",
    "\n",
    "def create_dataset(src_dicts, dst_dicts, prompt):\n",
    "    \"\"\"\n",
    "    Creates a Hugging-Face datasets DatasetDict that can be used for training.\n",
    "\n",
    "    src_dict --- A dict {essay_id: essay_text} containing the source (uncorrected) texts.\n",
    "    dst_dict --- A dict {essay_id: essay_text} containg the target (corrected) texts.\n",
    "    split --- The split to create the dict from.\n",
    "    prompt --- The prompt to insert before each source text.\n",
    "\n",
    "    \"\"\"\n",
    "    return DatasetDict(\n",
    "        {\n",
    "            \"train\": create_dataset_dict(src_dicts, dst_dicts, \"train\", prompt),\n",
    "            \"validation\": create_dataset_dict(src_dicts, dst_dicts, \"dev\", prompt),\n",
    "            \"test\": create_dataset_dict(src_dicts, dst_dicts, \"test\", prompt),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_dataset = create_dataset(source_text_dicts, minimal_text_dicts, minimal_prompt)\n",
    "\n",
    "fluency_dataset = create_dataset(source_text_dicts, fluency_text_dicts, fluency_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"source\"], text_target=dataset[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Both Datasets\n",
    "\n",
    "This creates tokenized versions of both the minimal-edit dataset and the fluency-edit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_minimal_dataset = minimal_dataset.map(tokenize_function)\n",
    "\n",
    "tokenized_fluency_dataset = fluency_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of each tokenized sequence to ensure they are all within 4096 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Datasets to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dir = \"datasets\"\n",
    "makedirs(datasets_dir, exist_ok=True)\n",
    "\n",
    "minimal_dataset_path = path.join(datasets_dir, \"minimal\")\n",
    "fluency_dataset_path = path.join(datasets_dir, \"fluency\")\n",
    "\n",
    "tokenized_minimal_dataset.save_to_disk(minimal_dataset_path)\n",
    "tokenized_fluency_dataset.save_to_disk(fluency_dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
