{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Datasets\n",
    "\n",
    "This notebook reads all SweLL-gold source files and converts them into transformers-compatible Datasets.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Ensure you have the SweLL-gold corpus downloaded and a `.env` file in the repository root with a path to the SweLL-gold directory, i.e:\n",
    "```\n",
    "SWELL_DIR=<PATH TO SWELL DIRECTORY>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv, path, makedirs\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions to Read Corpus Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SweLL-directory path from .env file\n",
    "load_dotenv()\n",
    "swell_dir = getenv(\"SWELL_DIR\")\n",
    "\n",
    "\n",
    "def get_file_paths(file_type):\n",
    "    \"\"\"\n",
    "    Gets the paths to the files containing dev, test, and train split for the given file_type.\n",
    "    The file_type must be one of: orig (source), ref1 (minimal edit), or ref2 (fluency edit)\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure file_type is correct\n",
    "    assert file_type in [\"orig\", \"ref1\", \"ref2\"]\n",
    "\n",
    "    base_name = f\"sv-swell_gold-{file_type}\"\n",
    "    splits = [\"dev\", \"test\", \"train\"]\n",
    "    return {\n",
    "        split: path.join(swell_dir, f\"{base_name}-{split}.md\")\n",
    "        for split in splits\n",
    "    }\n",
    "\n",
    "\n",
    "def md_to_dict(md):\n",
    "    \"\"\"\n",
    "    From the multigec-2025-data-providers repo:\n",
    "    - https://github.com/spraakbanken/multigec-2025-data-providers\n",
    "\n",
    "    Parse shared task format into a dictionary where keys are essay IDs\n",
    "    and values are essay texts.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    md --- a string with the content of a shared task Markdown file.\n",
    "    \"\"\"\n",
    "    essay_dict = {}\n",
    "    for essay in md.split(\"### essay_id = \")[1:]:\n",
    "        (essay_id, text) = essay.split(\"\\n\", maxsplit=1)\n",
    "        essay_dict[essay_id] = text.strip(\"\\n\")\n",
    "    return essay_dict\n",
    "\n",
    "\n",
    "def read_file_to_dict(fp):\n",
    "    \"\"\"\n",
    "    Reads the file path into a dictionary, by using the function md_to_dict.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    fp --- The File path to read.\n",
    "    \"\"\"\n",
    "    with open(fp) as f:\n",
    "        md = f.read()\n",
    "    return md_to_dict(md)\n",
    "\n",
    "\n",
    "def read_files_to_dicts(file_type):\n",
    "    \"\"\"\n",
    "    Reads the corresponding files for file_type (orig, ref1, ref2) into a dictionary with keys dev, test, and train and values {essay_id: essay_text}.\n",
    "    \"\"\"\n",
    "    file_paths = get_file_paths(file_type)\n",
    "    return {\n",
    "        split: read_file_to_dict(file_path)\n",
    "        for split, file_path in file_paths.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files into Dictionaries\n",
    "\n",
    "Each file is read into a dictionary on the format:\n",
    "\n",
    "```python\n",
    "{split: {essay_id: essay_text}}\n",
    "```\n",
    "\n",
    "Where split is in [\"dev\", \"train\", \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text_dicts = read_files_to_dicts(\"orig\")\n",
    "minimal_text_dicts = read_files_to_dicts(\"ref1\")\n",
    "fluency_text_dicts = read_files_to_dicts(\"ref2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset-Creation Helper Functions\n",
    "\n",
    "These helper functions are used to create the Hugging-Face datasets Dataset dictionaries that are used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_source_target_dict(src_dict, dst_dict):\n",
    "    \"\"\"\n",
    "    Create Hugging-Face transformers-compatible source-target dicts.\n",
    "    First ensures both dicts have equal keys for a correct mapping.\n",
    "    Then returns a list of dictionaries with keys source and target and values source text and target text.\n",
    "\n",
    "    ```python\n",
    "    [{source:source_text, target:target_text}]\n",
    "    ```\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    src_dict --- A dict {essay_id: essay_text} containing the source (uncorrected) texts.\n",
    "    dst_dict --- A dict {essay_id: essay_text} containg the target (corrected) texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure both input dictionaries have equal keys\n",
    "    assert src_dict.keys() == dst_dict.keys()\n",
    "\n",
    "    essay_ids = src_dict.keys()\n",
    "\n",
    "    return [\n",
    "        {\"source\": src_dict[essay_id], \"target\": dst_dict[essay_id]}\n",
    "        for essay_id in essay_ids\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_dataset_dict(src_dict, dst_dict, split):\n",
    "    \"\"\"\n",
    "    Creates a Hugging-Face Dataset dict with the same split from the src_dict and dst_dict.\n",
    "\n",
    "    Arguments:\n",
    "    src_dict --- A dict {essay_id: essay_text} containing the source (uncorrected) texts.\n",
    "    dst_dict --- A dict {essay_id: essay_text} containg the target (corrected) texts.\n",
    "    split --- The split to create the dict from.\n",
    "    \"\"\"\n",
    "    return Dataset.from_list(\n",
    "        create_source_target_dict(src_dict[split], dst_dict[split])\n",
    "    )\n",
    "\n",
    "\n",
    "def create_dataset(src_dicts, dst_dicts):\n",
    "    \"\"\"\n",
    "    Creates a Hugging-Face datasets DatasetDict that can be used for training.\n",
    "\n",
    "    src_dict --- A dict {essay_id: essay_text} containing the source (uncorrected) texts.\n",
    "    dst_dict --- A dict {essay_id: essay_text} containg the target (corrected) texts.\n",
    "    split --- The split to create the dict from.\n",
    "\n",
    "    \"\"\"\n",
    "    return DatasetDict(\n",
    "        {\n",
    "            \"train\": create_dataset_dict(src_dicts, dst_dicts, \"train\"),\n",
    "            \"validation\": create_dataset_dict(src_dicts, dst_dicts, \"dev\"),\n",
    "            \"test\": create_dataset_dict(src_dicts, dst_dicts, \"test\"),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_dataset = create_dataset(source_text_dicts, minimal_text_dicts)\n",
    "fluency_dataset = create_dataset(source_text_dicts, fluency_text_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Datasets to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e0009949b44548be7fe63bbf5e2ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/402 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44006e4024c408498e0c3dff4426e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4922b594884c27bad086c3be130892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cdc071d3c6a4617bac36c6e88fafdc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/402 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113e4fd443e34933894b8423d23c63eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982674d9be7449f4a7aafe58511663af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets_dir = \"datasets\"\n",
    "makedirs(datasets_dir, exist_ok=True)\n",
    "\n",
    "minimal_dataset_path = path.join(datasets_dir, \"minimal\")\n",
    "fluency_dataset_path = path.join(datasets_dir, \"fluency\")\n",
    "\n",
    "minimal_dataset.save_to_disk(minimal_dataset_path)\n",
    "fluency_dataset.save_to_disk(fluency_dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
