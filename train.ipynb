{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune the Viking LLM\n",
    "\n",
    "This notebook fine-tunes the Viking LLM to perform GEC with regard to both minimal edits and fluency edit.\n",
    "\n",
    "Chose model-version and edit-version further down.\n",
    "\n",
    "## Imports\n",
    "\n",
    "Import all relevant packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import minimal_prompt, fluency_prompt\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from os import path, makedirs\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"GPU is not available for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "The `version` variable can be either \"minimal\" or \"fluency\".\n",
    "\n",
    "The `model_name` variable can be any Hugging Face model name, e.g \"LumiOpen/Viking-7B\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"minimal\"\n",
    "model_name = \"LumiOpen/Viking-7B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Version\n",
    "\n",
    "Verify that the value of `version` is valued and raise a `ValueError` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if version not in [\"minimal\", \"fluency\"]:\n",
    "    raise ValueError(\"Invalid version.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Setup Quantization Config\n",
    "\n",
    "-   Train the LLM with the normalized float 4 `nf4` data type.\n",
    "-   Do not double quantize, i.e do not quantize the quantization constants.\n",
    "-   Perform computations in the brain-float 16 `bfloat16` data type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "\n",
    "Load the model with the above quantization config.\n",
    "\n",
    "The last two lines prepare the model for LoRA training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer and Data Collator\n",
    "\n",
    "The tokenizer converts the input text into tokens for the LLM to use.\n",
    "\n",
    "The data collator groups input essays into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Load Prompt\n",
    "\n",
    "Load the prompt corresponding to the `version` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {\"minimal\": minimal_prompt, \"fluency\": fluency_prompt}\n",
    "\n",
    "prompt = prompts[version]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Base Dataset\n",
    "\n",
    "Load the base (untokenized) dataset from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = path.join(\"datasets\", version)\n",
    "dataset = load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset\n",
    "\n",
    "Prepend `prompt` to each source sequence and tokenize both source and target sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [prompt + example for example in examples[\"source\"]]\n",
    "    targets = [example for example in examples[\"target\"]]\n",
    "    return tokenizer(inputs, text_target=targets, max_length=4096, padding=\"max_length\")\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "### Setup LoRA\n",
    "\n",
    "Important arguments are explained in the list below:\n",
    "\n",
    "- Use rank $ r = 128 $ to replace each weight matrix $ W \\in \\mathbb{R}^{ N \\times M } $ with two smaller matrices $ A \\in \\mathbb{R}^{ N \\times r } $ and $ B \\in \\mathbb{R}^{ r \\times M } $, where $ r \\ll \\min ( N, M ) $ .\n",
    "- Use $ \\alpha = 64 $ to scale the matrix-product $ A B $ by the factor $ \\alpha / r $.\n",
    "- Target the projection matrices $ W^{ Q } $, $ W^{ V } $, and $ W^{ K } $.\n",
    "\n",
    "Then use the LoRA config to prepare the LLM for PEFT training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=64,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training Arguments\n",
    "\n",
    "Begin by setting up number of epochs and batch size.\n",
    "\n",
    "Then calculate how often to evaluate, which is twice per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 4\n",
    "\n",
    "dataset_size = len(tokenized_dataset[\"train\"])\n",
    "steps_per_epoch = (dataset_size * epochs) // batch_size\n",
    "num_eval_steps = steps_per_epoch // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Model Directory\n",
    "\n",
    "Define the directory to save the trained model at, e.g. `./models/Viking-7B/<version>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label = model_name.split(\"/\")[1]\n",
    "model_dir = path.join(\"models\", model_label, version)\n",
    "makedirs(model_dir)  # Ensure directory exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training Arguments\n",
    "\n",
    "Important arguments are explained below:\n",
    "\n",
    "- Evalute every half epoch.\n",
    "- Use 8-bit AdamW optimizer.\n",
    "- Only compute evaluation loss.\n",
    "- Use a constant learning-rate of $ 5 \\times 10^{ - 5 } $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    num_train_epochs=epochs,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=num_eval_steps,\n",
    "    prediction_loss_only=True,\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    learning_rate=5e-5,\n",
    "    bf16=True,\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_strategy=\"epoch\",\n",
    "    overwrite_output_dir=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Train the model.\n",
    "\n",
    "Training loss is logged at every training step, since the dataset is so small.\n",
    "\n",
    "Model snapshots are saved at the end of every epoch and the model is also saved at the end of traning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
