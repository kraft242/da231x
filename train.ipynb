{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune the Viking LLM\n",
    "\n",
    "This notebook fine-tunes the Viking LLM to perform GEC with regard to both minimal edits and fluency edit.\n",
    "\n",
    "Chose model-version and edit-version further down.\n",
    "\n",
    "## Imports\n",
    "\n",
    "Import all relevant packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import minimal_prompt, fluency_prompt\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from os import path, makedirs\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"GPU is not available for training!\")\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "The `version` variable can be either \"minimal\" or \"fluency\".\n",
    "\n",
    "The `model_name` variable can be any Hugging Face model name, e.g \"LumiOpen/Viking-7B\".\n",
    "\n",
    "The `model_label` variable is the part after the slash, e.g \"Viking-7B\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"minimal\"\n",
    "model_name = \"LumiOpen/Viking-7B\"\n",
    "model_label = model_name.split(\"/\")[1]\n",
    "MAX_LENGTH = 4096  # Well above the longest token sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Version\n",
    "\n",
    "Verify that the value of `version` is valued and raise a `ValueError` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if version not in [\"minimal\", \"fluency\"]:\n",
    "    raise ValueError(\"Invalid version.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Setup Quantization Config\n",
    "\n",
    "-   Train the LLM with the normalized float 4 `nf4` data type.\n",
    "-   Do not double quantize, i.e do not quantize the quantization constants.\n",
    "-   Perform computations in the brain-float 16 `bfloat16` data type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "\n",
    "Load the model with the above quantization config.\n",
    "\n",
    "The last two lines prepare the model for LoRA training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer and Data Collator\n",
    "\n",
    "The tokenizer converts the input text into tokens for the LLM to use.\n",
    "\n",
    "The data collator groups input essays into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Load Prompt\n",
    "\n",
    "Load the prompt corresponding to the `version` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {\"minimal\": minimal_prompt, \"fluency\": fluency_prompt}\n",
    "\n",
    "prompt = prompts[version]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Base Dataset\n",
    "\n",
    "Load the base (untokenized) dataset from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = path.join(\"datasets\", version)\n",
    "dataset = load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset\n",
    "\n",
    "Wrap each source-target input pair in a prompt, which looks like this:\n",
    "\n",
    "```markdown\n",
    "### Instruktioner:\n",
    "<CORRECTION_PROMPT>\n",
    "\n",
    "### Indata:\n",
    "<SOURCE_TEXT>\n",
    "\n",
    "### Utdata:\n",
    "<TARGET_TEXT>\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [\n",
    "        f\"### Instruktioner:\\n{prompt}\\n\\n### Indata:\\n{source}\\n\\n### Utdata:\\n{target}\\n\"\n",
    "        for source, target in zip(examples[\"source\"], examples[\"target\"])\n",
    "    ]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        examples[\"target\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "### Setup LoRA\n",
    "\n",
    "Important arguments are explained in the list below:\n",
    "\n",
    "- Use rank $ r = 128 $ to replace each weight matrix $ W \\in \\mathbb{R}^{ N \\times M } $ with two smaller matrices $ A \\in \\mathbb{R}^{ N \\times r } $ and $ B \\in \\mathbb{R}^{ r \\times M } $, where $ r \\ll \\min ( N, M ) $ .\n",
    "- Use $ \\alpha = 64 $ to scale the matrix-product $ A B $ by the factor $ \\alpha / r $.\n",
    "- Target the projection matrices $ W^{ Q } $, $ W^{ V } $, and $ W^{ K } $.\n",
    "\n",
    "Then use the LoRA config to prepare the LLM for PEFT training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=64,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training Arguments\n",
    "\n",
    "Begin by setting up number of epochs and batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Model Directory\n",
    "\n",
    "Define the directory to save the trained model in as `./models/<model_label>/<version>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = path.join(\"models\", model_label, version)\n",
    "makedirs(model_dir, exist_ok=True)  # Ensure directory exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training Arguments\n",
    "\n",
    "Important arguments are explained below:\n",
    "\n",
    "- Use 8-bit AdamW optimizer.\n",
    "- Use a constant learning-rate of $ 5 \\times 10^{ - 5 } $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    num_train_epochs=epochs,\n",
    "    logging_steps=1,\n",
    "    prediction_loss_only=True,\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    learning_rate=5e-5,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    label_names=[\"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Train the model.\n",
    "\n",
    "Training loss is logged at every training step, since the dataset is so small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Locally\n",
    "\n",
    "Save the model and tokenizer locally.\n",
    "\n",
    "Do **not** push to hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(model_dir, push_to_hub=False)\n",
    "tokenizer.save_pretrained(model_dir, push_to_hub=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
