{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_csv = \"scores.csv\"\n",
    "df = pd.read_csv(scores_csv)\n",
    "print(df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_values(df, column):\n",
    "    return df[column].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMAL = \"minimal\"\n",
    "FLUENCY = \"fluency\"\n",
    "VIKING = \"Viking\"\n",
    "UAM_CSI = \"UAM-CSI\"\n",
    "GLEU = \"GLEU\"\n",
    "PRECISION = \"Precision\"\n",
    "RECALL = \"Recall\"\n",
    "F05 = \"F0.5\"\n",
    "SCRIBENDI_SCORE = \"Scribendi Score\"\n",
    "versions = [MINIMAL, FLUENCY]\n",
    "teams = [VIKING, UAM_CSI]\n",
    "metrics = [GLEU, PRECISION, RECALL, F05, SCRIBENDI_SCORE]\n",
    "continuous_metrics = [GLEU, PRECISION, RECALL, F05]\n",
    "ordinal_metrics = [SCRIBENDI_SCORE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=[\"System\", \"Correction Style\", \"Essay ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_team_version_metric(team, version, metric):\n",
    "    return df[(df[\"System\"] == team) & (df[\"Correction Style\"] == version)][\n",
    "        metric\n",
    "    ].to_numpy()\n",
    "\n",
    "\n",
    "def extract_team_version(team, version):\n",
    "    return {\n",
    "        metric: extract_team_version_metric(team, version, metric) for metric in metrics\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_team(team):\n",
    "    return {version: extract_team_version(team, version) for version in versions}\n",
    "\n",
    "\n",
    "values = {team: extract_team(team) for team in teams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "\n",
    "def perform_test(metric, version):\n",
    "    viking_values = values[VIKING][version][metric]\n",
    "    uamcsi_values = values[UAM_CSI][version][metric]\n",
    "    if metric in continuous_metrics:\n",
    "        return ttest_rel(viking_values, uamcsi_values)\n",
    "    elif metric in ordinal_metrics:\n",
    "        return wilcoxon(viking_values - uamcsi_values)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric {metric}\")\n",
    "\n",
    "\n",
    "results = []\n",
    "for metric in metrics:\n",
    "    for version in versions:\n",
    "        test_result = perform_test(metric, version)\n",
    "        p_value = test_result.pvalue\n",
    "        is_significant = p_value < alpha\n",
    "        result_dict = {\n",
    "            \"metric\": metric,\n",
    "            \"version\": version,\n",
    "            \"p_value\": p_value,\n",
    "            \"significant\": is_significant,\n",
    "        }\n",
    "        results.append(result_dict)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
