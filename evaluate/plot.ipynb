{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228bbc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "from os import path, makedirs\n",
    "import json\n",
    "from scipy.stats import wilcoxon, shapiro, normaltest, ttest_rel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79919a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "systems_type = CategoricalDtype(\n",
    "    categories=[\"UAM-CSI\", \"Viking-7B\", \"Viking-13B\"], ordered=True\n",
    ")\n",
    "styles_type = CategoricalDtype(categories=[\"minimal\", \"fluency\"], ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbedd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_csv_file = \"scores.csv\"\n",
    "auto_df = pd.read_csv(scores_csv_file)\n",
    "\n",
    "\n",
    "auto_df_long = auto_df.melt(\n",
    "    id_vars=[\"essay_id\", \"correction_style\", \"system\"],\n",
    "    value_vars=[\"gleu\", \"precision\", \"recall\", \"f0.5\", \"scribendi_score\"],\n",
    "    var_name=\"metric\",\n",
    "    value_name=\"score\",\n",
    ")\n",
    "\n",
    "print(auto_df_long.info())\n",
    "\n",
    "styles = auto_df_long[\"correction_style\"].unique().tolist()\n",
    "teams = auto_df_long[\"system\"].unique().tolist()\n",
    "\n",
    "output_metrics = {\n",
    "    \"grammaticality\": \"SOME: Grammaticality\",\n",
    "    \"fluency\": \"SOME: Fluency\",\n",
    "    \"meaning_preservation\": \"SOME: Meaning Preservation\",\n",
    "    \"manual_evaluation\": \"SOME: Total\",\n",
    "    \"gleu\": \"GLEU\",\n",
    "    \"precision\": \"ERRANT: Precision\",\n",
    "    \"recall\": \"ERRANT: Recall\",\n",
    "    \"f0.5\": \"ERRANT: $ \\\\text{F}_{0.5} $-Score\",\n",
    "    \"scribendi_score\": \"Scribendi Score\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457203d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_evaluation_dir = \"manual_evaluation/\"\n",
    "d_key = \"evaluations\"\n",
    "\n",
    "manual_eval_dicts = []\n",
    "\n",
    "for team in teams:\n",
    "    team_dir = path.join(manual_evaluation_dir, team)\n",
    "    for style in styles:\n",
    "        style_file_name = f\"{style}.json\"\n",
    "        style_file_path = path.join(team_dir, style_file_name)\n",
    "        with open(style_file_path) as f:\n",
    "            metric_df = json.load(f)\n",
    "        scores = metric_df[d_key]\n",
    "        for d in scores:\n",
    "            total = 0\n",
    "            for metric in [\"grammaticality\", \"fluency\", \"meaning_preservation\"]:\n",
    "                manual_eval_dicts.append(\n",
    "                    {\n",
    "                        \"essay_id\": d[\"id\"],\n",
    "                        \"correction_style\": style,\n",
    "                        \"system\": team,\n",
    "                        \"metric\": metric,\n",
    "                        \"score\": d[metric],\n",
    "                    }\n",
    "                )\n",
    "                total += d[metric]\n",
    "            manual_eval_dicts.append(\n",
    "                {\n",
    "                    \"essay_id\": d[\"id\"],\n",
    "                    \"correction_style\": style,\n",
    "                    \"system\": team,\n",
    "                    \"metric\": \"manual_evaluation\",\n",
    "                    \"score\": total / 3,\n",
    "                }\n",
    "            )\n",
    "\n",
    "manual_df_long = pd.DataFrame(manual_eval_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37169ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [auto_df_long, manual_df_long]\n",
    "df_long = pd.concat(dfs)\n",
    "df_long[\"system\"] = df_long[\"system\"].astype(systems_type)\n",
    "df_long[\"correction_style\"] = df_long[\"correction_style\"].astype(styles_type)\n",
    "print(df_long.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f33f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_ids = df_long[\"essay_id\"].unique().tolist()\n",
    "\n",
    "\n",
    "essay_id_subs = {essay_id: i for i, essay_id in enumerate(essay_ids, 1)}\n",
    "\n",
    "df_long[\"essay_id\"] = df_long[\"essay_id\"].map(essay_id_subs)\n",
    "print(df_long.info())\n",
    "\n",
    "metrics = df_long[\"metric\"].unique().tolist()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa26d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_legend_label(label):\n",
    "    match label:\n",
    "        case \"minimal\":\n",
    "            return \"Minimal Edits\"\n",
    "        case \"fluency\":\n",
    "            return \"Fluency Edits\"\n",
    "        case _:\n",
    "            return output_metrics.get(label, label).replace(\"_\", \" \").title()\n",
    "\n",
    "\n",
    "def normalize_file_name(file_name):\n",
    "    return file_name.lower().replace(\".\", \"\").replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "plots_dir = \"plots/\"\n",
    "makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "metric_types = {\n",
    "    \"continuous\": [\n",
    "        \"gleu\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"f0.5\",\n",
    "        \"manual_evaluation\",\n",
    "    ],\n",
    "    \"discrete\": [\n",
    "        \"scribendi_score\",\n",
    "        \"grammaticality\",\n",
    "        \"fluency\",\n",
    "        \"meaning_preservation\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def get_minmax(v):\n",
    "    return v.min(), v.max()\n",
    "\n",
    "\n",
    "def is_even(n):\n",
    "    return n % 2 == 0\n",
    "\n",
    "\n",
    "offset = 0.2\n",
    "\n",
    "\n",
    "for metric in metrics:\n",
    "\n",
    "    metric_df = df_long[df_long[\"metric\"] == metric]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    sns.violinplot(\n",
    "        metric_df,\n",
    "        y=\"system\",\n",
    "        x=\"score\",\n",
    "        cut=0,\n",
    "        hue=\"correction_style\",\n",
    "        inner=None,\n",
    "        density_norm=\"area\",\n",
    "    )\n",
    "    grouped = metric_df.groupby([\"system\", \"correction_style\"], observed=False)[\"score\"]\n",
    "    means = grouped.mean()\n",
    "    medians = grouped.median()\n",
    "\n",
    "    ys = [i // 2 - offset if is_even(i) else i // 2 + offset for i in range(len(means))]\n",
    "    plt.scatter(\n",
    "        y=ys,\n",
    "        x=means,\n",
    "        marker=\"s\",\n",
    "        color=\"black\",\n",
    "        edgecolors=\"white\",\n",
    "        zorder=3,\n",
    "        label=\"Mean\",\n",
    "    )\n",
    "    plt.scatter(\n",
    "        y=ys,\n",
    "        x=medians,\n",
    "        marker=\"o\",\n",
    "        color=\"white\",\n",
    "        edgecolors=\"black\",\n",
    "        zorder=3,\n",
    "        label=\"Median\",\n",
    "    )\n",
    "\n",
    "    if metric in metric_types[\"discrete\"]:\n",
    "        # Ensure axes show integers\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    ax.set(xlabel=output_metrics[metric], ylabel=\"System\")\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles, map(format_legend_label, labels))\n",
    "    sns.move_legend(\n",
    "        ax,\n",
    "        \"lower center\",\n",
    "        ncol=2,\n",
    "        bbox_to_anchor=(0.5, 1),\n",
    "        frameon=True,\n",
    "    )\n",
    "    file_name = f\"{normalize_file_name(metric)}.png\"\n",
    "    file_path = path.join(plots_dir, file_name)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_long.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    metric_df = df_long[df_long[\"metric\"] == metric]\n",
    "    ax = sns.lineplot(\n",
    "        metric_df, x=\"essay_id\", y=\"score\", hue=\"system\", style=\"correction_style\"\n",
    "    )\n",
    "    ax.set(xlabel=\"Essay ID\", ylabel=output_metrics[metric])\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles, map(format_legend_label, labels))\n",
    "    sns.move_legend(\n",
    "        ax,\n",
    "        \"upper center\",\n",
    "        bbox_to_anchor=(1.35, 1),\n",
    "        ncol=1,\n",
    "        frameon=True,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    metric_df = df_long[df_long[\"metric\"] == metric]\n",
    "    g = sns.FacetGrid(\n",
    "        metric_df,\n",
    "        col=\"correction_style\",\n",
    "        row=\"system\",\n",
    "        margin_titles=True,\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "    g.map_dataframe(sns.histplot, x=\"score\", bins=8)\n",
    "\n",
    "    # Set axis labels and titles\n",
    "    g.set_axis_labels(\"Score\", \"Count\")\n",
    "    g.set_titles(row_template=\"{row_name}\", col_template=\"{col_name}\")\n",
    "\n",
    "    plt.title(metric)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be07c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b28ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spread(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "\n",
    "summary = (\n",
    "    df_long.groupby([\"correction_style\", \"system\", \"metric\"])\n",
    "    .agg(\n",
    "        mean=(\"score\", \"mean\"),\n",
    "        median=(\"score\", \"median\"),\n",
    "        min=(\"score\", \"min\"),\n",
    "        max=(\"score\", \"max\"),\n",
    "        spread=(\"score\", get_spread),\n",
    "        # std=(\"score\", \"std\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "metric_dfs = {\n",
    "    metric: group.drop(columns=\"metric\")\n",
    "    .sort_values(by=[\"correction_style\", \"system\"], ascending=[True, True])\n",
    "    .reset_index(drop=True)\n",
    "    for metric, group in summary.groupby(\"metric\")\n",
    "}\n",
    "\n",
    "\n",
    "output_styles = {\n",
    "    \"minimal\": \"Minimal\",\n",
    "    \"fluency\": \"Fluency\",\n",
    "}\n",
    "\n",
    "output_headers = {\n",
    "    \"correction_style\": \"Edit Style\",\n",
    "    \"system\": \"System\",\n",
    "}\n",
    "\n",
    "mu = r\"\\( \\mu \\)\"\n",
    "sigma = r\"\\( \\sigma \\)\"\n",
    "\n",
    "for metric, metric_df in metric_dfs.items():\n",
    "    metric_df.rename(columns=output_headers, inplace=True)\n",
    "    metric_df[\"Edit Style\"] = metric_df[\"Edit Style\"].map(output_styles)\n",
    "    metric_df.set_index([\"Edit Style\", \"System\"], inplace=True)\n",
    "\n",
    "latex_args = {\n",
    "    \"sparse_index\": True,\n",
    "    \"convert_css\": True,\n",
    "    \"clines\": \"skip-last;data\",\n",
    "    \"hrules\": True,\n",
    "    \"column_format\": None,\n",
    "    \"siunitx\": True,\n",
    "    \"multicol_align\": \"c\",\n",
    "}\n",
    "\n",
    "tables_dir = \"tables/\"\n",
    "makedirs(tables_dir, exist_ok=True)\n",
    "summary_dir = path.join(tables_dir, \"summary\")\n",
    "makedirs(summary_dir, exist_ok=True)\n",
    "\n",
    "highlight = {\n",
    "    \"max\": \"background-color: kth-lightblue40\",\n",
    "    \"min\": \"background-color: kth-lightred40\",\n",
    "}\n",
    "\n",
    "\n",
    "def float_formatter(x):\n",
    "    return f\"\\\\num{{{x:.2f}}}\"\n",
    "\n",
    "\n",
    "for metric, metric_df in metric_dfs.items():\n",
    "    print(metric)\n",
    "\n",
    "    latex = (\n",
    "        # metric_df.style.highlight_max(\n",
    "        #    subset=[\"mean\", \"median\", \"max\", \"min\"], props=highlight[\"max\"], axis=0\n",
    "        # )\n",
    "        # .highlight_min(subset=sigma, props=\"font-weight: bold\", axis=0)\n",
    "        metric_df.style.highlight_min(props=highlight[\"min\"], axis=0)\n",
    "        .highlight_max(props=highlight[\"max\"], axis=0)\n",
    "        .format(formatter=float_formatter)\n",
    "        .to_latex(\n",
    "            **latex_args,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    file_name = f\"{normalize_file_name(metric)}.tex\"\n",
    "    file_path = path.join(summary_dir, file_name)\n",
    "    with open(file_path, \"w+\") as f:\n",
    "        f.write(latex)\n",
    "\n",
    "    print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa9e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_metrics = metric_types[\"discrete\"]\n",
    "\n",
    "discrete_df = df_long[df_long[\"metric\"].isin(discrete_metrics)]\n",
    "\n",
    "\n",
    "def int_formatter(x):\n",
    "    return f\"\\\\num{{{x}}}\"\n",
    "\n",
    "\n",
    "for metric in discrete_metrics:\n",
    "    metric_df = (\n",
    "        discrete_df[discrete_df[\"metric\"] == metric]\n",
    "        .drop(columns=[\"metric\", \"essay_id\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    metric_df.rename(columns=output_headers, inplace=True)\n",
    "    metric_df[\"Edit Style\"] = metric_df[\"Edit Style\"].map(output_styles)\n",
    "    metric_df[\"score\"] = metric_df[\"score\"].astype(int)\n",
    "    mean = metric_df.groupby([\"Edit Style\", \"System\"], observed=False)[\"score\"].mean()\n",
    "    median = metric_df.groupby([\"Edit Style\", \"System\"], observed=False)[\n",
    "        \"score\"\n",
    "    ].median()\n",
    "    metric_df = metric_df.pivot_table(\n",
    "        index=[\"Edit Style\", \"System\"],\n",
    "        columns=\"score\",\n",
    "        aggfunc=\"size\",\n",
    "        fill_value=0,\n",
    "        observed=False,\n",
    "    )\n",
    "    metric_df[\"mean\"] = mean\n",
    "    metric_df[\"median\"] = median\n",
    "    cols = [\"mean\", \"median\"] + list(metric_df.columns[:-2])\n",
    "    metric_df = metric_df[cols]\n",
    "    metric_df.columns = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (\"\", col) if col in [\"mean\", \"median\"] else (\"Score Count\", col)\n",
    "            for col in metric_df.columns\n",
    "        ]\n",
    "    )\n",
    "    print(metric)\n",
    "    print(metric_df)\n",
    "\n",
    "    latex = (\n",
    "        metric_df.style.highlight_min(props=highlight[\"min\"], axis=0)\n",
    "        .highlight_max(props=highlight[\"max\"], axis=0)\n",
    "        .format(formatter=float_formatter, subset=[(\"\", \"mean\"), (\"\", \"median\")])\n",
    "        .format(formatter=int_formatter, subset=[\"Score Count\"])\n",
    "        .to_latex(\n",
    "            **latex_args,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    file_name = f\"{normalize_file_name(metric)}.tex\"\n",
    "    file_path = path.join(summary_dir, file_name)\n",
    "    with open(file_path, \"w+\") as f:\n",
    "        f.write(latex)\n",
    "\n",
    "    print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_types[\"continuous\"].remove(\"manual_evaluation\")\n",
    "metric_types[\"discrete\"].append(\"manual_evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a50c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_level = 0.05\n",
    "\n",
    "dists = []\n",
    "grouped = df_long.groupby(\n",
    "    [\"system\", \"correction_style\", \"metric\"], observed=False\n",
    ")\n",
    "\n",
    "for metric in metric_types[\"continuous\"]:\n",
    "    for team in teams:\n",
    "        for style in styles:\n",
    "            scores = grouped.get_group((team, style, metric))[\n",
    "                \"score\"].to_numpy()\n",
    "\n",
    "            shapiro_stat, shapiro_p = shapiro(scores)\n",
    "            normaltest_stat, normaltest_p = normaltest(scores)\n",
    "\n",
    "            dists.append(\n",
    "                {\n",
    "                    \"metric\": metric,\n",
    "                    \"team\": team,\n",
    "                    \"style\": style,\n",
    "                    # \"shapiro_stat\": shapiro_stat,\n",
    "                    \"sp\": shapiro_p,\n",
    "                    \"sn\": shapiro_p > significance_level,\n",
    "                    # \"normaltest_stat\": normaltest_stat,\n",
    "                    \"np\": normaltest_p,\n",
    "                    \"nn\": normaltest_p > significance_level,\n",
    "                }\n",
    "            )\n",
    "\n",
    "dist_df = pd.DataFrame(dists)\n",
    "print(dist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c7b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped = df_long.groupby([\"system\", \"correction_style\", \"metric\"])\n",
    "baseline = \"UAM-CSI\"\n",
    "vikings = [t for t in teams if t != baseline]\n",
    "\n",
    "\n",
    "test_results = []\n",
    "for team in vikings:\n",
    "    for metric in metrics:\n",
    "        for style in styles:\n",
    "            keys = [team, baseline]\n",
    "            args = [(k, style, metric) for k in keys]\n",
    "            scores = [\n",
    "                grouped.get_group(arg).sort_values(by=\"essay_id\")[\"score\"].to_numpy()\n",
    "                for arg in args\n",
    "            ]\n",
    "\n",
    "            if metric in metric_types[\"continuous\"]:\n",
    "                stat, p_value = ttest_rel(scores[0], scores[1], alternative=\"greater\")\n",
    "            else:\n",
    "                diffs = np.around(scores[0] - scores[1], 3)\n",
    "                stat, p_value = wilcoxon(diffs, alternative=\"greater\")\n",
    "\n",
    "            test_results.append(\n",
    "                {\n",
    "                    \"team\": team,\n",
    "                    \"style\": style,\n",
    "                    \"metric\": metric,\n",
    "                    \"statistic\": stat,\n",
    "                    \"p_value\": p_value,\n",
    "                    \"significant\": p_value < significance_level,\n",
    "                }\n",
    "            )\n",
    "\n",
    "test_results_df = pd.DataFrame(test_results)\n",
    "# sub = test_results_df[[\"team\", \"style\", \"metric\", \"p_value\", \"significant\"]]\n",
    "test_results_df = test_results_df.sort_values(\n",
    "    by=[\"metric\", \"style\", \"team\"], ascending=[True, False, False]\n",
    ")\n",
    "# print(test_results_df)\n",
    "\n",
    "pretty_test_results_df = test_results_df.copy()\n",
    "cols = [\"metric\", \"style\", \"team\", \"p_value\"]\n",
    "pretty_test_results_df = pretty_test_results_df[cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "output_headers = {\n",
    "    \"style\": \"Edit Style\",\n",
    "    \"team\": \"System\",\n",
    "    \"p_value\": \"$ p $-value\",\n",
    "    \"significant\": \"Significant\",\n",
    "    \"metric\": \"Metric\",\n",
    "    \"statistic\": \"Statistic\",\n",
    "}\n",
    "\n",
    "\n",
    "renamed = test_results_df.rename(columns=output_headers)\n",
    "renamed[\"Edit Style\"] = renamed[\"Edit Style\"].map(output_styles)\n",
    "renamed[\"Metric\"] = renamed[\"Metric\"].map(output_metrics)\n",
    "\n",
    "pivot = renamed.pivot_table(\n",
    "    index=[output_headers[\"metric\"]],\n",
    "    columns=[output_headers[\"style\"], output_headers[\"team\"]],\n",
    "    values=[output_headers[\"p_value\"]],\n",
    ")\n",
    "\n",
    "latex_args[\"column_format\"] = None\n",
    "latex_args[\"siunitx\"] = True\n",
    "\n",
    "\n",
    "def scientific_formatter(x):\n",
    "    if pd.notnull(x):\n",
    "        pretty = f\"{x:.2e}\"\n",
    "        return f\"\\\\num{{{pretty}}}\"\n",
    "    return f\"\\\\text{{NaN}}\"\n",
    "\n",
    "green = \"background-color: kth-lightgreen\"\n",
    "latex = (\n",
    "    pivot.style.map(\n",
    "        lambda p: (green if float(p) < significance_level else \"\"),\n",
    "        subset=[output_headers[\"p_value\"]],\n",
    "    )\n",
    "    .format(formatter=scientific_formatter)\n",
    "    .to_latex(**latex_args)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a0d49",
   "metadata": {},
   "source": [
    "## Test Multi Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4397008",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_test_results_df = test_results_df.copy()\n",
    "cols = [\"metric\", \"style\", \"team\", \"p_value\"]\n",
    "pretty_test_results_df = pretty_test_results_df[cols].reset_index(drop=True)\n",
    "\n",
    "renamed = pretty_test_results_df.rename(columns=output_headers)\n",
    "renamed[\"Edit Style\"] = renamed[\"Edit Style\"].map(output_styles)\n",
    "renamed[\"Metric\"] = renamed[\"Metric\"].map(output_metrics)\n",
    "\n",
    "renamed[[\"Metric\", \"Submetric\"]] = renamed[\"Metric\"].str.split(\":\", expand=True)\n",
    "renamed[\"Submetric\"] = renamed[\"Submetric\"].str.strip().fillna(\"-\")\n",
    "\n",
    "# Reorder the columns\n",
    "index_cols = [\"Metric\", \"Submetric\", \"Edit Style\", \"System\"]\n",
    "\n",
    "\n",
    "# Change the index to index_cols and make multiple levels\n",
    "renamed.set_index(index_cols, inplace=True)\n",
    "\n",
    "metric_order = CategoricalDtype(\n",
    "    categories=[\n",
    "        output_metrics[\"gleu\"],\n",
    "        \"ERRANT\",\n",
    "        output_metrics[\"scribendi_score\"],\n",
    "        \"SOME\",\n",
    "    ],\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "submetrics_raw = [\n",
    "    \"precision\",\n",
    "    \"recall\",\n",
    "    \"f0.5\",\n",
    "    \"grammaticality\",\n",
    "    \"fluency\",\n",
    "    \"meaning_preservation\",\n",
    "    \"manual_evaluation\",\n",
    "]\n",
    "\n",
    "submetrics = [\n",
    "    output_metrics[submetric].split(\":\")[1].strip() for submetric in submetrics_raw\n",
    "] + [\"-\"]\n",
    "\n",
    "submetric_order = CategoricalDtype(\n",
    "    categories=submetrics,\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "style_order = CategoricalDtype(\n",
    "    categories=[\"Minimal\", \"Fluency\"],\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "system_order = CategoricalDtype(\n",
    "    categories=[\"Viking-7B\", \"Viking-13B\"],\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "\n",
    "renamed.index = renamed.index.set_levels(\n",
    "    renamed.index.levels[0].astype(metric_order),\n",
    "    level=0,\n",
    ")\n",
    "renamed.index = renamed.index.set_levels(\n",
    "    renamed.index.levels[1].astype(submetric_order),\n",
    "    level=1,\n",
    ")\n",
    "\n",
    "renamed.index = renamed.index.set_levels(\n",
    "    renamed.index.levels[2].astype(style_order),\n",
    "    level=2,\n",
    ")\n",
    "renamed.index = renamed.index.set_levels(\n",
    "    renamed.index.levels[3].astype(system_order),\n",
    "    level=3,\n",
    ")\n",
    "renamed = renamed.sort_index()\n",
    "\n",
    "latex_args[\"column_format\"] = None\n",
    "latex_args[\"siunitx\"] = True\n",
    "\n",
    "\n",
    "def scientific_formatter(x):\n",
    "    if pd.notnull(x):\n",
    "        pretty = f\"{x:.2e}\"\n",
    "        return f\"\\\\num{{{pretty}}}\"\n",
    "    return f\"\\\\text{{NaN}}\"\n",
    "\n",
    "\n",
    "green = \"background-color: kth-lightgreen\"\n",
    "latex = (\n",
    "    renamed.style.map(\n",
    "        lambda p: (green if float(p) < significance_level else \"\"),\n",
    "    )\n",
    "    .format(formatter=scientific_formatter)\n",
    "    .to_latex(**latex_args)\n",
    ")\n",
    "\n",
    "\n",
    "print(latex)\n",
    "file_name = \"test_results.tex\"\n",
    "file_path = path.join(tables_dir, file_name)\n",
    "with open(file_path, \"w+\") as f:\n",
    "    f.write(latex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
