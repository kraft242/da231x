{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16495f20",
   "metadata": {},
   "source": [
    "# Plots and Tables\n",
    "\n",
    "This notebook creates plots and tables\n",
    "\n",
    "## Imports\n",
    "\n",
    "Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228bbc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "from os import path, makedirs\n",
    "from scipy.stats import wilcoxon, shapiro, normaltest, ttest_rel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d76c6",
   "metadata": {},
   "source": [
    "## Ordering\n",
    "\n",
    "Setup the ordering of categories for the tables in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79919a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = {\n",
    "    \"Edit Style\": CategoricalDtype(categories=[\"Minimal\", \"Fluency\"], ordered=True),\n",
    "    \"Metric\": CategoricalDtype(\n",
    "        categories=[\"GLEU\", \"ERRANT\", \"Scribendi Score\", \"SOME\"], ordered=True\n",
    "    ),\n",
    "    \"Submetric\": CategoricalDtype(\n",
    "        categories=[\n",
    "            \"-\",\n",
    "            \"Precision\",\n",
    "            \"Recall\",\n",
    "            \"$\\\\text{F}_{0.5}$-Score\",\n",
    "            \"Grammaticality\",\n",
    "            \"Fluency\",\n",
    "            \"Meaning Preservation\",\n",
    "            \"Total\",\n",
    "        ],\n",
    "        ordered=True,\n",
    "    ),\n",
    "    \"System\": CategoricalDtype(\n",
    "        categories=[\"UAM-CSI\", \"Viking-7B\", \"Viking-13B\"], ordered=True\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f24a1",
   "metadata": {},
   "source": [
    "## Read File\n",
    "\n",
    "Read the raw CSV file into a pandas `DataFrame` and setup variables for holding category values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbedd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_csv_file = \"scores_long.csv\"\n",
    "df = pd.read_csv(scores_csv_file)\n",
    "df = df.fillna(\"-\")\n",
    "\n",
    "for col, dtype in orders.items():\n",
    "    df[col] = df[col].astype(dtype)\n",
    "\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "columns = df.columns\n",
    "essay_ids = df[\"Essay ID\"].unique()\n",
    "styles = df[\"Edit Style\"].unique()\n",
    "metrics = [\n",
    "    tuple(row)\n",
    "    for row in df[[\"Metric\", \"Submetric\"]].drop_duplicates().to_numpy().tolist()\n",
    "]\n",
    "systems = df[\"System\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057bb249",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Define helper functions to be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_label(label):\n",
    "    if label in metrics:\n",
    "        major, minor = label\n",
    "        return f\"{major}: {minor}\" if minor != \"-\" else major\n",
    "    elif label in styles:\n",
    "        return f\"{label} Edits\"\n",
    "    return label\n",
    "\n",
    "\n",
    "metric_map = {\n",
    "    (\"GLEU\", \"-\"): \"gleu\",\n",
    "    (\"ERRANT\", \"Precision\"): \"errant_precision\",\n",
    "    (\"ERRANT\", \"Recall\"): \"errant_recall\",\n",
    "    (\"ERRANT\", \"$\\\\text{F}_{0.5}$-Score\"): \"errant_f05\",\n",
    "    (\"Scribendi Score\", \"-\"): \"scribendi_score\",\n",
    "    (\"SOME\", \"Grammaticality\"): \"some_grammaticality\",\n",
    "    (\"SOME\", \"Fluency\"): \"some_fluency\",\n",
    "    (\"SOME\", \"Meaning Preservation\"): \"some_meaning_preservation\",\n",
    "    (\"SOME\", \"Total\"): \"some_total\",\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_metric(metric):\n",
    "    try:\n",
    "        return metric_map[metric]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Unknown metric: {metric}\")\n",
    "\n",
    "\n",
    "def get_image_file_name(metric):\n",
    "    return f\"{normalize_metric(metric)}.png\"\n",
    "\n",
    "\n",
    "def normalize_file_name(file_name):\n",
    "    return file_name.lower().replace(\".\", \"\").replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "def get_minmax(v):\n",
    "    return v.min(), v.max()\n",
    "\n",
    "\n",
    "def is_even(n):\n",
    "    return n % 2 == 0\n",
    "\n",
    "\n",
    "def get_metric_df(metric):\n",
    "    major, minor = metric\n",
    "    if pd.isna(minor):\n",
    "        return df[df[\"Metric\"] == major]\n",
    "    return df[(df[\"Metric\"] == major) & (df[\"Submetric\"] == minor)]\n",
    "\n",
    "\n",
    "tables_dir = \"tables/\"\n",
    "makedirs(tables_dir, exist_ok=True)\n",
    "summary_dir = path.join(tables_dir, \"summary\")\n",
    "makedirs(summary_dir, exist_ok=True)\n",
    "plots_dir = \"plots/\"\n",
    "makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_latex_table(latex, file_name):\n",
    "    file_path = path.join(tables_dir, file_name)\n",
    "    with open(file_path, \"w+\") as f:\n",
    "        f.write(latex)\n",
    "\n",
    "\n",
    "def get_spread(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "\n",
    "def get_tex_file_name(metric):\n",
    "    normalized = normalize_metric(metric)\n",
    "    return f\"{normalized}.tex\"\n",
    "\n",
    "\n",
    "def float_formatter(x):\n",
    "    return f\"\\\\num{{{x:.2f}}}\"\n",
    "\n",
    "\n",
    "def int_formatter(x):\n",
    "    return f\"\\\\num{{{x}}}\"\n",
    "\n",
    "\n",
    "metric_types = {\n",
    "    \"continuous\": [\n",
    "        (\"GLEU\", \"-\"),\n",
    "        (\"ERRANT\", \"Precision\"),\n",
    "        (\"ERRANT\", \"Recall\"),\n",
    "        (\"ERRANT\", \"$\\\\text{F}_{0.5}$-Score\"),\n",
    "        (\"SOME\", \"Total\"),\n",
    "    ],\n",
    "    \"discrete\": [\n",
    "        (\"Scribendi Score\", \"-\"),\n",
    "        (\"SOME\", \"Grammaticality\"),\n",
    "        (\"SOME\", \"Fluency\"),\n",
    "        (\"SOME\", \"Meaning Preservation\"),\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8b92f",
   "metadata": {},
   "source": [
    "## Plot Scores\n",
    "\n",
    "Plot the scores for each metric in a violin plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f647503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0.2\n",
    "\n",
    "for metric in metrics:\n",
    "\n",
    "    metric_df = get_metric_df(metric)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    sns.violinplot(\n",
    "        metric_df,\n",
    "        y=\"System\",\n",
    "        x=\"Score\",\n",
    "        cut=0,\n",
    "        hue=\"Edit Style\",\n",
    "        inner=None,\n",
    "        density_norm=\"area\",\n",
    "    )\n",
    "\n",
    "    violin_handles, violin_labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    grouped = metric_df.groupby([\"System\", \"Edit Style\"], observed=False)[\"Score\"]\n",
    "    means = grouped.mean()\n",
    "    medians = grouped.median()\n",
    "    n = len(means)\n",
    "\n",
    "    offsets = np.array([-offset if is_even(i) else offset for i in range(n)])\n",
    "    ys = np.array([i // 2 for i in range(len(means))]) + offsets\n",
    "\n",
    "    plt.scatter(\n",
    "        y=ys,\n",
    "        x=means,\n",
    "        marker=\"s\",\n",
    "        color=\"black\",\n",
    "        edgecolors=\"white\",\n",
    "        zorder=3,\n",
    "        label=\"Mean\",\n",
    "    )\n",
    "    plt.scatter(\n",
    "        y=ys,\n",
    "        x=medians,\n",
    "        marker=\"o\",\n",
    "        color=\"white\",\n",
    "        edgecolors=\"black\",\n",
    "        zorder=3,\n",
    "        label=\"Median\",\n",
    "    )\n",
    "\n",
    "    # Ensure axes show integers for discrete metrics\n",
    "    if metric in metric_types[\"discrete\"]:\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # Custom handles for Statistics\n",
    "    stat_handles = [\n",
    "        Line2D(\n",
    "            [0],\n",
    "            [0],\n",
    "            marker=\"s\",\n",
    "            color=\"black\",\n",
    "            label=\"Mean\",\n",
    "            markerfacecolor=\"black\",\n",
    "            markeredgecolor=\"white\",\n",
    "            linestyle=\"\",\n",
    "        ),\n",
    "        Line2D(\n",
    "            [0],\n",
    "            [0],\n",
    "            marker=\"o\",\n",
    "            color=\"white\",\n",
    "            label=\"Median\",\n",
    "            markerfacecolor=\"white\",\n",
    "            markeredgecolor=\"black\",\n",
    "            linestyle=\"\",\n",
    "        ),\n",
    "    ]\n",
    "    stat_labels = [h.get_label() for h in stat_handles]\n",
    "\n",
    "    handles = [\n",
    "        Line2D([], [], linestyle=\"none\"),\n",
    "        *violin_handles,\n",
    "        Line2D([], [], linestyle=\"none\"),\n",
    "        *stat_handles,\n",
    "    ]\n",
    "\n",
    "    labels = [\n",
    "        \"Edit Style\",\n",
    "        *violin_labels,\n",
    "        \"Statistics\",\n",
    "        *stat_labels,\n",
    "    ]\n",
    "\n",
    "    # Create the legend\n",
    "    ax.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(1, 1),\n",
    "        frameon=True,\n",
    "        ncol=1,\n",
    "        handletextpad=1,\n",
    "    )\n",
    "\n",
    "    ax.set(xlabel=format_label(metric), ylabel=\"System\")\n",
    "    file_name = get_image_file_name(metric)\n",
    "    file_path = path.join(plots_dir, file_name)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc446e3",
   "metadata": {},
   "source": [
    "## Score Distribution\n",
    "\n",
    "Plot essay-wise scores for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    metric_df = get_metric_df(metric)\n",
    "\n",
    "    ax = sns.lineplot(\n",
    "        metric_df, x=\"Essay ID\", y=\"Score\", hue=\"System\", style=\"Edit Style\"\n",
    "    )\n",
    "    ax.set(xlabel=\"Essay ID\", ylabel=format_label(metric))\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles, map(format_label, labels))\n",
    "    sns.move_legend(\n",
    "        ax,\n",
    "        \"upper left\",\n",
    "        bbox_to_anchor=(1, 1),\n",
    "        ncol=1,\n",
    "        frameon=True,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6560a",
   "metadata": {},
   "source": [
    "## Score Distributions\n",
    "\n",
    "Plot the distributions of the different scores in histograms to see if they are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    metric_df = get_metric_df(metric)\n",
    "    g = sns.FacetGrid(\n",
    "        metric_df,\n",
    "        col=\"Edit Style\",\n",
    "        row=\"System\",\n",
    "        margin_titles=True,\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "    g.map_dataframe(sns.histplot, x=\"Score\", bins=8)\n",
    "\n",
    "    g.set_axis_labels(\"Score\", \"Count\")\n",
    "    g.set_titles(row_template=\"{row_name}\", col_template=\"{col_name}\")\n",
    "\n",
    "    plt.title(format_label(metric))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df132a",
   "metadata": {},
   "source": [
    "## Tables\n",
    "\n",
    "### Helpers \n",
    "\n",
    "Create helper functions and variables to use when creating the metric tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b28ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_args = {\n",
    "    \"sparse_index\": True,\n",
    "    \"convert_css\": True,\n",
    "    \"clines\": \"skip-last;data\",\n",
    "    \"hrules\": True,\n",
    "    \"column_format\": None,\n",
    "    \"siunitx\": True,\n",
    "    \"multicol_align\": \"c\",\n",
    "}\n",
    "\n",
    "\n",
    "highlight = {\n",
    "    \"max\": \"background-color: kth-lightblue40\",\n",
    "    \"min\": \"background-color: kth-lightred40\",\n",
    "}\n",
    "\n",
    "\n",
    "uparrow = r\"$\\uparrow$\"\n",
    "downarrow = r\"$\\downarrow$\"\n",
    "aggregated_column_names = {\n",
    "    \"mean\": \"mean\" + uparrow,\n",
    "    \"median\": \"median\" + uparrow,\n",
    "    \"min\": \"min\" + uparrow,\n",
    "    \"max\": \"max\" + uparrow,\n",
    "    \"spread\": \"spread\" + downarrow,\n",
    "}\n",
    "\n",
    "hi_better = [\"mean\", \"median\", \"min\", \"max\"]\n",
    "lo_better = [\"spread\"]\n",
    "\n",
    "hi_better = [aggregated_column_names[l] for l in hi_better]\n",
    "lo_better = [aggregated_column_names[l] for l in lo_better]\n",
    "\n",
    "\n",
    "def summarize_continuous_metric(metric):\n",
    "    metric_df = get_metric_df(metric)\n",
    "\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [\n",
    "            metric_df[\"Edit Style\"].unique(),\n",
    "            metric_df[\"System\"].unique(),\n",
    "        ],\n",
    "        names=[\"Edit Style\", \"System\"],\n",
    "    )\n",
    "\n",
    "    summary_df = (\n",
    "        metric_df.groupby([\"Edit Style\", \"System\"], observed=False)\n",
    "        .agg(\n",
    "            mean=(\"Score\", \"mean\"),\n",
    "            median=(\"Score\", \"median\"),\n",
    "            min=(\"Score\", \"min\"),\n",
    "            max=(\"Score\", \"max\"),\n",
    "            spread=(\"Score\", get_spread),\n",
    "        )\n",
    "        .reindex(index)\n",
    "    )\n",
    "\n",
    "    summary_df = summary_df.rename(columns=aggregated_column_names)\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def summarize_discrete_metric(metric):\n",
    "    metric_df = get_metric_df(metric)\n",
    "\n",
    "    scores = metric_df.groupby([\"Edit Style\", \"System\"], observed=False)[\"Score\"]\n",
    "\n",
    "    summary = metric_df.pivot_table(\n",
    "        index=[\"Edit Style\", \"System\"],\n",
    "        columns=\"Score\",\n",
    "        aggfunc=\"size\",\n",
    "        fill_value=0,\n",
    "        observed=False,\n",
    "    ).assign(mean=scores.mean(), median=scores.median())\n",
    "\n",
    "    columns = [\"mean\", \"median\"] + list(summary.columns[:-2])\n",
    "    summary = summary[columns]\n",
    "\n",
    "    summary.columns = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (\"\", col) if col in [\"mean\", \"median\"] else (\"Score Count\", int(col))\n",
    "            for col in summary.columns\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def get_continuous_metric_latex(metric):\n",
    "    return (\n",
    "        summarize_continuous_metric(metric)\n",
    "        .style.highlight_min(props=highlight[\"min\"], subset=hi_better, axis=0)\n",
    "        .highlight_max(props=highlight[\"max\"], subset=hi_better, axis=0)\n",
    "        .highlight_min(props=highlight[\"max\"], subset=lo_better, axis=0)\n",
    "        .highlight_max(props=highlight[\"min\"], subset=lo_better, axis=0)\n",
    "        .format(formatter=float_formatter)\n",
    "        .to_latex(\n",
    "            **latex_args,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def get_discrete_metric_latex(metric):\n",
    "    return (\n",
    "        summarize_discrete_metric(metric)\n",
    "        .style.highlight_min(props=highlight[\"min\"], axis=0)\n",
    "        .highlight_max(props=highlight[\"max\"], axis=0)\n",
    "        .format(formatter=float_formatter, subset=[(\"\", \"mean\"), (\"\", \"median\")])\n",
    "        .format(formatter=int_formatter, subset=[\"Score Count\"])\n",
    "        .to_latex(\n",
    "            **latex_args,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def get_metric_latex(metric):\n",
    "    if metric in metric_types[\"continuous\"]:\n",
    "        return get_continuous_metric_latex(metric)\n",
    "    elif metric in metric_types[\"discrete\"]:\n",
    "        return get_discrete_metric_latex(metric)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric type for {metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39080fcd",
   "metadata": {},
   "source": [
    "### Create tables\n",
    "\n",
    "Create the metric-wise tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f6ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    latex = get_metric_latex(metric)\n",
    "\n",
    "    print(latex)\n",
    "    file_name = get_tex_file_name(metric)\n",
    "    save_latex_table(latex, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd128dc",
   "metadata": {},
   "source": [
    "## Change SOME: Total\n",
    "\n",
    "We treat the SOME: Total metric as continuous in the plots, but not in the statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_types[\"continuous\"].remove((\"SOME\", \"Total\"))\n",
    "metric_types[\"discrete\"].append((\"SOME\", \"Total\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673d4717",
   "metadata": {},
   "source": [
    "## Distribution Tests\n",
    "\n",
    "Perform statistical tests to see whether the continuous metrics are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a50c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_level = 0.05\n",
    "\n",
    "dists = []\n",
    "\n",
    "for metric in metric_types[\"continuous\"]:\n",
    "    metric_df = get_metric_df(metric)\n",
    "    for team in systems:\n",
    "        team_df = metric_df[metric_df[\"System\"] == team]\n",
    "        for style in styles:\n",
    "            style_df = team_df[team_df[\"Edit Style\"] == style]\n",
    "            scores = style_df[\"Score\"].to_numpy()\n",
    "\n",
    "            shapiro_stat, shapiro_p = shapiro(scores)\n",
    "            normaltest_stat, normaltest_p = normaltest(scores)\n",
    "\n",
    "            dists.append(\n",
    "                {\n",
    "                    \"metric\": format_label(metric),\n",
    "                    \"team\": team,\n",
    "                    \"style\": style,\n",
    "                    \"sp\": shapiro_p,\n",
    "                    \"sn\": shapiro_p > significance_level,\n",
    "                    \"np\": normaltest_p,\n",
    "                    \"nn\": normaltest_p > significance_level,\n",
    "                }\n",
    "            )\n",
    "\n",
    "dist_df = pd.DataFrame(dists)\n",
    "display(dist_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee2695",
   "metadata": {},
   "source": [
    "## Statistical-Test Helpers\n",
    "\n",
    "Define helper functions and variables for use in the statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d02b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = \"UAM-CSI\"\n",
    "vikings = [t for t in systems if t != baseline]\n",
    "\n",
    "\n",
    "def get_alternate_hypothesis(hypothesis):\n",
    "    def is_odd(n):\n",
    "        return n % 2 != 0\n",
    "\n",
    "    return \"greater\" if is_odd(hypothesis) else \"less\"\n",
    "\n",
    "\n",
    "def format_hypothesis(hypothesis):\n",
    "    return f\"$H_{{{hypothesis}}}$\"\n",
    "\n",
    "\n",
    "def perform_statistical_test(metric, scores, alternative):\n",
    "    if metric in metric_types[\"continuous\"]:\n",
    "        return ttest_rel(scores[0], scores[1], alternative=alternative)\n",
    "    diffs = np.around(scores[0] - scores[1], 3)\n",
    "    return wilcoxon(diffs, alternative=alternative)\n",
    "\n",
    "\n",
    "grouped = df.groupby([\"System\", \"Edit Style\", \"Metric\", \"Submetric\"], observed=False)\n",
    "\n",
    "\n",
    "def perform_statistical_tests(hypothesis):\n",
    "    test_results = []\n",
    "\n",
    "    for team in vikings:\n",
    "        for metric in metrics:\n",
    "            for style in styles:\n",
    "                keys = [team, baseline]\n",
    "                args = [(k, style, *metric) for k in keys]\n",
    "                scores = [\n",
    "                    grouped.get_group(arg)\n",
    "                    .sort_values(by=\"Essay ID\")[\"Score\"]\n",
    "                    .to_numpy()\n",
    "                    for arg in args\n",
    "                ]\n",
    "\n",
    "                stat, p_value = perform_statistical_test(\n",
    "                    metric, scores, get_alternate_hypothesis(hypothesis)\n",
    "                )\n",
    "\n",
    "                major, minor = metric\n",
    "                test_results.append(\n",
    "                    {\n",
    "                        \"System\": team,\n",
    "                        \"Edit Style\": style,\n",
    "                        \"Metric\": major,\n",
    "                        \"Submetric\": minor,\n",
    "                        \"statistic\": stat,\n",
    "                        \"$p$-value\": p_value,\n",
    "                        \"Hypothesis\": format_hypothesis(hypothesis),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a0d49",
   "metadata": {},
   "source": [
    "## Perform Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e0c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = perform_statistical_tests(1)\n",
    "neg = perform_statistical_tests(2)\n",
    "test_results = pos + neg\n",
    "test_results_df = pd.DataFrame(test_results)\n",
    "display(test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4ed8f8",
   "metadata": {},
   "source": [
    "## Save Essay-Wise Statistical-Test Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_results(test_results_df, columns):\n",
    "    # Select relevant columns and reset index\n",
    "    df = test_results_df[columns]  # .reset_index(drop=True)\n",
    "\n",
    "    pivot_index = [\n",
    "        c for c in [\"Metric\", \"Submetric\", \"Edit Style\", \"System\"] if c in df.columns\n",
    "    ]\n",
    "\n",
    "    pivoted = df.pivot(index=pivot_index, columns=[\"Hypothesis\"], values=[\"$p$-value\"])\n",
    "\n",
    "    pivoted_reset = pivoted.reset_index()\n",
    "    pivoted_sorted = pivoted_reset.sort_values(\n",
    "        by=pivot_index,\n",
    "        key=lambda col: col if col.name not in orders else col.astype(orders[col.name]),\n",
    "    )\n",
    "    pivoted_sorted = pivoted_sorted.set_index(pivot_index)\n",
    "\n",
    "    return pivoted_sorted\n",
    "\n",
    "\n",
    "def highlight_significant(p):\n",
    "    return \"background-color: kth-lightgreen\" if float(p) < significance_level else \"\"\n",
    "\n",
    "\n",
    "def generate_latex_table(df, latex_args, formatter):\n",
    "    return (\n",
    "        df.style.map(highlight_significant)\n",
    "        .format(formatter=formatter)\n",
    "        .to_latex(**latex_args)\n",
    "    )\n",
    "\n",
    "\n",
    "def scientific_formatter(x):\n",
    "    if pd.notnull(x):\n",
    "        pretty = f\"{x:.2e}\"\n",
    "        return f\"\\\\num{{{pretty}}}\"\n",
    "    return f\"\\\\text{{NaN}}\"\n",
    "\n",
    "\n",
    "# Define constants\n",
    "columns = [\"Metric\", \"Submetric\", \"Edit Style\", \"System\", \"$p$-value\", \"Hypothesis\"]\n",
    "\n",
    "renamed = prepare_test_results(test_results_df, columns)\n",
    "\n",
    "latex = generate_latex_table(renamed, latex_args, scientific_formatter)\n",
    "\n",
    "file_name = \"test_results.tex\"\n",
    "save_latex_table(latex, file_name)\n",
    "\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9675f9ae",
   "metadata": {},
   "source": [
    "## Compare Viking-Based Systems\n",
    "\n",
    "\n",
    "## Compare Model Sizes\n",
    "\n",
    "Compare the two model sizes with the same edit style to see whether Viking-7B or Viking-13B is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "viking_results = []\n",
    "for metric in metrics:\n",
    "    major, minor = metric\n",
    "    for style in styles:\n",
    "        args = [(v, style, *metric) for v in vikings]\n",
    "        scores = [\n",
    "            grouped.get_group(arg).sort_values(by=\"Essay ID\")[\"Score\"].to_numpy()\n",
    "            for arg in args\n",
    "        ]\n",
    "        \"\"\"Explanation of the hypotheses:\n",
    "        3: Viking-7B > Viking-13B\n",
    "        4: Viking-7B < Viking-13B\n",
    "        \"\"\"\n",
    "        hypotheses = [3, 4]\n",
    "        for hypothesis in hypotheses:\n",
    "            stat, p_value = perform_statistical_test(\n",
    "                metric, scores, get_alternate_hypothesis(hypothesis)\n",
    "            )\n",
    "            viking_results.append(\n",
    "                {\n",
    "                    \"Edit Style\": style,\n",
    "                    \"Metric\": major,\n",
    "                    \"Submetric\": minor,\n",
    "                    \"statistic\": stat,\n",
    "                    \"$p$-value\": p_value,\n",
    "                    \"Hypothesis\": format_hypothesis(hypothesis),\n",
    "                }\n",
    "            )\n",
    "\n",
    "viking_results_df = pd.DataFrame(viking_results)\n",
    "# Prepare the test results DataFrame\n",
    "columns = [\"Metric\", \"Submetric\", \"Edit Style\", \"$p$-value\", \"Hypothesis\"]\n",
    "renamed = prepare_test_results(viking_results_df, columns)\n",
    "display(renamed)\n",
    "\n",
    "# Generate the LaTeX table\n",
    "latex = generate_latex_table(renamed, latex_args, scientific_formatter)\n",
    "\n",
    "# Save the LaTeX table to a file\n",
    "file_name = \"viking_pairwise_test_results.tex\"\n",
    "save_latex_table(latex, file_name)\n",
    "\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895003cc",
   "metadata": {},
   "source": [
    "### Compare Edit Styles\n",
    "\n",
    "\n",
    "Compare the two edit styles with the same model size to see whether minimal edits or fluency edits are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_results = []\n",
    "\n",
    "for metric in metrics:\n",
    "    major, minor = metric\n",
    "    for v in vikings:\n",
    "        args = [(v, style, *metric) for style in styles]\n",
    "        scores = [\n",
    "            grouped.get_group(arg).sort_values(by=\"Essay ID\")[\"Score\"].to_numpy()\n",
    "            for arg in args\n",
    "        ]\n",
    "        \"\"\"Explanation of the hypotheses:\n",
    "        5: Minimal > Fluency\n",
    "        6: Minimal < Fluency\n",
    "        \"\"\"\n",
    "        hypotheses = [5, 6]\n",
    "        for hypothesis in hypotheses:\n",
    "            stat, p_value = perform_statistical_test(\n",
    "                metric, scores, get_alternate_hypothesis(hypothesis)\n",
    "            )\n",
    "            style_results.append(\n",
    "                {\n",
    "                    \"System\": v,\n",
    "                    \"Metric\": major,\n",
    "                    \"Submetric\": minor,\n",
    "                    \"$p$-value\": p_value,\n",
    "                    \"Hypothesis\": format_hypothesis(hypothesis),\n",
    "                }\n",
    "            )\n",
    "\n",
    "style_results_df = pd.DataFrame(style_results)\n",
    "# Prepare the test results DataFrame\n",
    "columns = [\"Metric\", \"Submetric\", \"System\", \"$p$-value\", \"Hypothesis\"]\n",
    "renamed = prepare_test_results(style_results_df, columns)\n",
    "display(renamed)\n",
    "\n",
    "# Generate the LaTeX table\n",
    "green = \"background-color: kth-lightgreen\"\n",
    "latex = generate_latex_table(renamed, latex_args, scientific_formatter)\n",
    "\n",
    "# Save the LaTeX table to a file\n",
    "file_name = \"viking_style_test_results.tex\"\n",
    "save_latex_table(latex, file_name)\n",
    "\n",
    "print(latex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
