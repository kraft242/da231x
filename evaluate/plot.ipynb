{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16495f20",
   "metadata": {},
   "source": [
    "# Plots and Tables\n",
    "\n",
    "This notebook creates plots and tables\n",
    "\n",
    "## Imports\n",
    "\n",
    "Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228bbc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "from os import path, makedirs\n",
    "from scipy.stats import wilcoxon, shapiro, normaltest, ttest_rel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d76c6",
   "metadata": {},
   "source": [
    "## Ordering\n",
    "\n",
    "Setup the ordering of categories for the tables in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79919a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = {\n",
    "    \"Edit Style\": CategoricalDtype(categories=[\"Minimal\", \"Fluency\"], ordered=True),\n",
    "    \"Metric\": CategoricalDtype(\n",
    "        categories=[\"GLEU\", \"ERRANT\", \"Scribendi Score\", \"SOME\"], ordered=True\n",
    "    ),\n",
    "    \"Submetric\": CategoricalDtype(\n",
    "        categories=[\n",
    "            \"-\",\n",
    "            \"Precision\",\n",
    "            \"Recall\",\n",
    "            \"$\\\\text{F}_{0.5}$-Score\",\n",
    "            \"Grammaticality\",\n",
    "            \"Fluency\",\n",
    "            \"Meaning Preservation\",\n",
    "            \"Total\",\n",
    "        ],\n",
    "        ordered=True,\n",
    "    ),\n",
    "    \"System\": CategoricalDtype(\n",
    "        categories=[\"UAM-CSI\", \"Viking-7B\", \"Viking-13B\"], ordered=True\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f24a1",
   "metadata": {},
   "source": [
    "## Read File\n",
    "\n",
    "Read the raw CSV file into a pandas `DataFrame` and setup variables for holding category values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbedd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_csv_file = \"scores_long.csv\"\n",
    "df = pd.read_csv(scores_csv_file)\n",
    "df = df.fillna(\"-\")\n",
    "\n",
    "for col, dtype in orders.items():\n",
    "    df[col] = df[col].astype(dtype)\n",
    "\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "columns = df.columns\n",
    "essay_ids = df[\"Essay ID\"].unique()\n",
    "styles = df[\"Edit Style\"].unique()\n",
    "metrics = [\n",
    "    tuple(row)\n",
    "    for row in df[[\"Metric\", \"Submetric\"]].drop_duplicates().to_numpy().tolist()\n",
    "]\n",
    "systems = df[\"System\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa26d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_label(label):\n",
    "    if label in metrics:\n",
    "        major, minor = label\n",
    "        if minor == \"-\":\n",
    "            return major\n",
    "        return f\"{major}: {minor}\"\n",
    "    elif label in styles:\n",
    "        return f\"{label} Edits\"\n",
    "    return label\n",
    "\n",
    "\n",
    "def normalize_metric(metric):\n",
    "    match metric:\n",
    "        case (\"GLEU\", \"-\"):\n",
    "            return \"gleu\"\n",
    "        case (\"ERRANT\", \"Precision\"):\n",
    "            return \"errant_precision\"\n",
    "        case (\"ERRANT\", \"Recall\"):\n",
    "            return \"errant_recall\"\n",
    "        case (\"ERRANT\", \"$\\\\text{F}_{0.5}$-Score\"):\n",
    "            return \"errant_f05\"\n",
    "        case (\"Scribendi Score\", \"-\"):\n",
    "            return \"scribendi_score\"\n",
    "        case (\"SOME\", \"Grammaticality\"):\n",
    "            return \"some_grammaticality\"\n",
    "        case (\"SOME\", \"Fluency\"):\n",
    "            return \"some_fluency\"\n",
    "        case (\"SOME\", \"Meaning Preservation\"):\n",
    "            return \"some_meaning_preservation\"\n",
    "        case (\"SOME\", \"Total\"):\n",
    "            return \"some_total\"\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown metric: {metric}\")\n",
    "\n",
    "\n",
    "def get_image_file_name(metric):\n",
    "    normalized = normalize_metric(metric)\n",
    "    return f\"{normalized}.png\"\n",
    "\n",
    "\n",
    "def normalize_file_name(file_name):\n",
    "    return file_name.lower().replace(\".\", \"\").replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "plots_dir = \"plots/\"\n",
    "makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "metric_types = {\n",
    "    \"continuous\": [\n",
    "        (\"GLEU\", \"-\"),\n",
    "        (\"ERRANT\", \"Precision\"),\n",
    "        (\"ERRANT\", \"Recall\"),\n",
    "        (\"ERRANT\", \"$\\\\text{F}_{0.5}$-Score\"),\n",
    "        (\"SOME\", \"Total\"),\n",
    "    ],\n",
    "    \"discrete\": [\n",
    "        (\"Scribendi Score\", \"-\"),\n",
    "        (\"SOME\", \"Grammaticality\"),\n",
    "        (\"SOME\", \"Fluency\"),\n",
    "        (\"SOME\", \"Meaning Preservation\"),\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def get_minmax(v):\n",
    "    return v.min(), v.max()\n",
    "\n",
    "\n",
    "def is_even(n):\n",
    "    return n % 2 == 0\n",
    "\n",
    "\n",
    "def get_metric_df(metric):\n",
    "    major, minor = metric\n",
    "    if pd.isna(minor):\n",
    "        return df[df[\"Metric\"] == major]\n",
    "    return df[(df[\"Metric\"] == major) & (df[\"Submetric\"] == minor)]\n",
    "\n",
    "\n",
    "offset = 0.2\n",
    "\n",
    "\n",
    "for metric in metrics:\n",
    "\n",
    "    metric_df = get_metric_df(metric)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    sns.violinplot(\n",
    "        metric_df,\n",
    "        y=\"System\",\n",
    "        x=\"Score\",\n",
    "        cut=0,\n",
    "        hue=\"Edit Style\",\n",
    "        inner=None,\n",
    "        density_norm=\"area\",\n",
    "    )\n",
    "\n",
    "    violin_handles, violin_labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    grouped = metric_df.groupby([\"System\", \"Edit Style\"], observed=False)[\"Score\"]\n",
    "    means = grouped.mean()\n",
    "    medians = grouped.median()\n",
    "\n",
    "    ys = [i // 2 - offset if is_even(i) else i // 2 + offset for i in range(len(means))]\n",
    "    plt.scatter(\n",
    "        y=ys,\n",
    "        x=means,\n",
    "        marker=\"s\",\n",
    "        color=\"black\",\n",
    "        edgecolors=\"white\",\n",
    "        zorder=3,\n",
    "        label=\"Mean\",\n",
    "    )\n",
    "    plt.scatter(\n",
    "        y=ys,\n",
    "        x=medians,\n",
    "        marker=\"o\",\n",
    "        color=\"white\",\n",
    "        edgecolors=\"black\",\n",
    "        zorder=3,\n",
    "        label=\"Median\",\n",
    "    )\n",
    "\n",
    "    # Ensure axes show integers for discrete metrics\n",
    "    if metric in metric_types[\"discrete\"]:\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # Custom handles for Statistics\n",
    "    stat_handles = [\n",
    "        Line2D(\n",
    "            [0],\n",
    "            [0],\n",
    "            marker=\"s\",\n",
    "            color=\"black\",\n",
    "            label=\"Mean\",\n",
    "            markerfacecolor=\"black\",\n",
    "            markeredgecolor=\"white\",\n",
    "            linestyle=\"\",\n",
    "        ),\n",
    "        Line2D(\n",
    "            [0],\n",
    "            [0],\n",
    "            marker=\"o\",\n",
    "            color=\"white\",\n",
    "            label=\"Median\",\n",
    "            markerfacecolor=\"white\",\n",
    "            markeredgecolor=\"black\",\n",
    "            linestyle=\"\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Combine with dummy headers\n",
    "    combined_handles = []\n",
    "    combined_labels = []\n",
    "\n",
    "    # Add Edit Style header (dummy)\n",
    "    combined_handles.append(Line2D([], [], linestyle=\"none\"))\n",
    "    combined_labels.append(\"Edit Style\")\n",
    "\n",
    "    # Add Edit Style entries\n",
    "    combined_handles.extend(violin_handles)\n",
    "    combined_labels.extend(violin_labels)\n",
    "\n",
    "    # Add Statistics header (dummy)\n",
    "    combined_handles.append(Line2D([], [], linestyle=\"none\"))\n",
    "    combined_labels.append(\"Statistics\")\n",
    "\n",
    "    # Add Statistics entries\n",
    "    combined_handles.extend(stat_handles)\n",
    "    combined_labels.extend([h.get_label() for h in stat_handles])\n",
    "\n",
    "    # Create the legend\n",
    "    ax.legend(\n",
    "        combined_handles,\n",
    "        combined_labels,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(1, 1),\n",
    "        frameon=True,\n",
    "        ncol=1,\n",
    "        handletextpad=1,\n",
    "    )\n",
    "\n",
    "    ax.set(xlabel=format_label(metric), ylabel=\"System\")\n",
    "    file_name = get_image_file_name(metric)\n",
    "    file_path = path.join(plots_dir, file_name)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    metric_df = get_metric_df(metric)\n",
    "\n",
    "    ax = sns.lineplot(\n",
    "        metric_df, x=\"Essay ID\", y=\"Score\", hue=\"System\", style=\"Edit Style\"\n",
    "    )\n",
    "    ax.set(xlabel=\"Essay ID\", ylabel=format_label(metric))\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles, map(format_label, labels))\n",
    "    sns.move_legend(\n",
    "        ax,\n",
    "        \"upper center\",\n",
    "        bbox_to_anchor=(1.35, 1),\n",
    "        ncol=1,\n",
    "        frameon=True,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    metric_df = get_metric_df(metric)\n",
    "    g = sns.FacetGrid(\n",
    "        metric_df,\n",
    "        col=\"Edit Style\",\n",
    "        row=\"System\",\n",
    "        margin_titles=True,\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "    g.map_dataframe(sns.histplot, x=\"Score\", bins=8)\n",
    "\n",
    "    # Set axis labels and titles\n",
    "    g.set_axis_labels(\"Score\", \"Count\")\n",
    "    g.set_titles(row_template=\"{row_name}\", col_template=\"{col_name}\")\n",
    "\n",
    "    plt.title(format_label(metric))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b28ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_latex_table(latex, file_name):\n",
    "    file_path = path.join(tables_dir, file_name)\n",
    "    with open(file_path, \"w+\") as f:\n",
    "        f.write(latex)\n",
    "\n",
    "\n",
    "def get_spread(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "\n",
    "def get_tex_file_name(metric):\n",
    "    normalized = normalize_metric(metric)\n",
    "    return f\"{normalized}.tex\"\n",
    "\n",
    "\n",
    "continuous_metrics = [m for m in metrics if m in metric_types[\"continuous\"]]\n",
    "\n",
    "latex_args = {\n",
    "    \"sparse_index\": True,\n",
    "    \"convert_css\": True,\n",
    "    \"clines\": \"skip-last;data\",\n",
    "    \"hrules\": True,\n",
    "    \"column_format\": None,\n",
    "    \"siunitx\": True,\n",
    "    \"multicol_align\": \"c\",\n",
    "}\n",
    "\n",
    "\n",
    "def float_formatter(x):\n",
    "    return f\"\\\\num{{{x:.2f}}}\"\n",
    "\n",
    "\n",
    "highlight = {\n",
    "    \"max\": \"background-color: kth-lightblue40\",\n",
    "    \"min\": \"background-color: kth-lightred40\",\n",
    "}\n",
    "\n",
    "tables_dir = \"tables/\"\n",
    "makedirs(tables_dir, exist_ok=True)\n",
    "summary_dir = path.join(tables_dir, \"summary\")\n",
    "makedirs(summary_dir, exist_ok=True)\n",
    "\n",
    "uparrow = r\"$\\uparrow$\"\n",
    "downarrow = r\"$\\downarrow$\"\n",
    "aggregated_column_names = {\n",
    "    \"mean\": \"mean\" + uparrow,\n",
    "    \"median\": \"median\" + uparrow,\n",
    "    \"min\": \"min\" + uparrow,\n",
    "    \"max\": \"max\" + uparrow,\n",
    "    \"spread\": \"spread\" + downarrow,\n",
    "}\n",
    "\n",
    "hi_better = [\"mean\", \"median\", \"min\", \"max\"]\n",
    "lo_better = [\"spread\"]\n",
    "\n",
    "hi_better = [aggregated_column_names[l] for l in hi_better]\n",
    "lo_better = [aggregated_column_names[l] for l in lo_better]\n",
    "\n",
    "for metric in continuous_metrics:\n",
    "    metric_df = get_metric_df(metric)\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [\n",
    "            metric_df[\"Edit Style\"].unique(),\n",
    "            metric_df[\"System\"].unique(),\n",
    "        ],\n",
    "        names=[\"Edit Style\", \"System\"],\n",
    "    )\n",
    "    summary = (\n",
    "        metric_df.groupby([\"Edit Style\", \"System\"], observed=False)\n",
    "        .agg(\n",
    "            mean=(\"Score\", \"mean\"),\n",
    "            median=(\"Score\", \"median\"),\n",
    "            min=(\"Score\", \"min\"),\n",
    "            max=(\"Score\", \"max\"),\n",
    "            spread=(\"Score\", get_spread),\n",
    "        )\n",
    "        .reindex(index)\n",
    "    )\n",
    "    summary = summary.rename(columns=aggregated_column_names)\n",
    "    print(metric)\n",
    "    latex = (\n",
    "        summary.style.highlight_min(props=highlight[\"min\"], subset=hi_better, axis=0)\n",
    "        .highlight_max(props=highlight[\"max\"], subset=hi_better, axis=0)\n",
    "        .highlight_min(props=highlight[\"max\"], subset=lo_better, axis=0)\n",
    "        .highlight_max(props=highlight[\"min\"], subset=lo_better, axis=0)\n",
    "        .format(formatter=float_formatter)\n",
    "        .to_latex(\n",
    "            **latex_args,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(latex)\n",
    "    file_name = get_tex_file_name(metric)\n",
    "    save_latex_table(latex, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa9e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_metrics = [m for m in metrics if m in metric_types[\"discrete\"]]\n",
    "\n",
    "\n",
    "def int_formatter(x):\n",
    "    return f\"\\\\num{{{x}}}\"\n",
    "\n",
    "\n",
    "for metric in discrete_metrics:\n",
    "    print(metric)\n",
    "    metric_df = get_metric_df(metric)\n",
    "\n",
    "    scores = metric_df.groupby([\"Edit Style\", \"System\"], observed=False)[\"Score\"]\n",
    "    mean = scores.mean()\n",
    "    median = scores.median()\n",
    "\n",
    "    summary = metric_df.pivot_table(\n",
    "        index=[\"Edit Style\", \"System\"],\n",
    "        columns=\"Score\",\n",
    "        aggfunc=\"size\",\n",
    "        fill_value=0,\n",
    "        observed=False,\n",
    "    )\n",
    "\n",
    "    summary[\"mean\"] = mean\n",
    "    summary[\"median\"] = median\n",
    "    columns = [\"mean\", \"median\"] + list(summary.columns[:-2])\n",
    "    summary = summary[columns]\n",
    "\n",
    "    summary.columns = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (\"\", col) if col in [\"mean\", \"median\"] else (\"Score Count\", int(col))\n",
    "            for col in summary.columns\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    latex = (\n",
    "        summary.style.highlight_min(props=highlight[\"min\"], axis=0)\n",
    "        .highlight_max(props=highlight[\"max\"], axis=0)\n",
    "        .format(formatter=float_formatter, subset=[(\"\", \"mean\"), (\"\", \"median\")])\n",
    "        .format(formatter=int_formatter, subset=[\"Score Count\"])\n",
    "        .to_latex(\n",
    "            **latex_args,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    file_name = get_tex_file_name(metric)\n",
    "    save_latex_table(latex, file_name)\n",
    "\n",
    "    print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_types[\"continuous\"].remove((\"SOME\", \"Total\"))\n",
    "metric_types[\"discrete\"].append((\"SOME\", \"Total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a50c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_level = 0.05\n",
    "\n",
    "dists = []\n",
    "# grouped = df_long.groupby([\"system\", \"correction_style\", \"metric\"], observed=False)\n",
    "\n",
    "for metric in metric_types[\"continuous\"]:\n",
    "    metric_df = get_metric_df(metric)\n",
    "    for team in systems:\n",
    "        team_df = metric_df[metric_df[\"System\"] == team]\n",
    "        for style in styles:\n",
    "            style_df = team_df[team_df[\"Edit Style\"] == style]\n",
    "            scores = style_df[\"Score\"].to_numpy()\n",
    "            # scores = grouped.get_group((team, style, metric))[\"score\"].to_numpy()\n",
    "\n",
    "            shapiro_stat, shapiro_p = shapiro(scores)\n",
    "            normaltest_stat, normaltest_p = normaltest(scores)\n",
    "\n",
    "            dists.append(\n",
    "                {\n",
    "                    \"metric\": format_label(metric),\n",
    "                    \"team\": team,\n",
    "                    \"style\": style,\n",
    "                    # \"shapiro_stat\": shapiro_stat,\n",
    "                    \"sp\": shapiro_p,\n",
    "                    \"sn\": shapiro_p > significance_level,\n",
    "                    # \"normaltest_stat\": normaltest_stat,\n",
    "                    \"np\": normaltest_p,\n",
    "                    \"nn\": normaltest_p > significance_level,\n",
    "                }\n",
    "            )\n",
    "\n",
    "dist_df = pd.DataFrame(dists)\n",
    "display(dist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d02b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = \"UAM-CSI\"\n",
    "vikings = [t for t in systems if t != baseline]\n",
    "\n",
    "\n",
    "def get_alternate_hypothesis(hypothesis):\n",
    "    def is_odd(n):\n",
    "        return n % 2 != 0\n",
    "\n",
    "    return \"greater\" if is_odd(hypothesis) else \"less\"\n",
    "\n",
    "\n",
    "def format_hypothesis(hypothesis):\n",
    "    return f\"$H_{{{hypothesis}}}$\"\n",
    "\n",
    "\n",
    "def perform_statistical_test(metric, scores, alternative):\n",
    "    if metric in metric_types[\"continuous\"]:\n",
    "        return ttest_rel(scores[0], scores[1], alternative=alternative)\n",
    "    diffs = np.around(scores[0] - scores[1], 3)\n",
    "    return wilcoxon(diffs, alternative=alternative)\n",
    "\n",
    "\n",
    "grouped = df.groupby([\"System\", \"Edit Style\", \"Metric\", \"Submetric\"], observed=False)\n",
    "\n",
    "\n",
    "def perform_statistical_tests(hypothesis):\n",
    "    test_results = []\n",
    "\n",
    "    for team in vikings:\n",
    "        for metric in metrics:\n",
    "            for style in styles:\n",
    "                keys = [team, baseline]\n",
    "                args = [(k, style, *metric) for k in keys]\n",
    "                scores = [\n",
    "                    grouped.get_group(arg)\n",
    "                    .sort_values(by=\"Essay ID\")[\"Score\"]\n",
    "                    .to_numpy()\n",
    "                    for arg in args\n",
    "                ]\n",
    "\n",
    "                stat, p_value = perform_statistical_test(\n",
    "                    metric, scores, get_alternate_hypothesis(hypothesis)\n",
    "                )\n",
    "\n",
    "                major, minor = metric\n",
    "                test_results.append(\n",
    "                    {\n",
    "                        \"System\": team,\n",
    "                        \"Edit Style\": style,\n",
    "                        \"Metric\": major,\n",
    "                        \"Submetric\": minor,\n",
    "                        \"statistic\": stat,\n",
    "                        \"$p$-value\": p_value,\n",
    "                        \"Hypothesis\": format_hypothesis(hypothesis),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a0d49",
   "metadata": {},
   "source": [
    "## Test Multi Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e0c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = perform_statistical_tests(1)\n",
    "neg = perform_statistical_tests(2)\n",
    "test_results = pos + neg\n",
    "test_results_df = pd.DataFrame(test_results)\n",
    "display(test_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_results(test_results_df, columns):\n",
    "    # Select relevant columns and reset index\n",
    "    df = test_results_df[columns]  # .reset_index(drop=True)\n",
    "\n",
    "    pivot_index = [\n",
    "        c for c in [\"Metric\", \"Submetric\", \"Edit Style\", \"System\"] if c in df.columns\n",
    "    ]\n",
    "\n",
    "    pivoted = df.pivot(index=pivot_index, columns=[\"Hypothesis\"], values=[\"$p$-value\"])\n",
    "\n",
    "    pivoted_reset = pivoted.reset_index()\n",
    "    pivoted_sorted = pivoted_reset.sort_values(\n",
    "        by=pivot_index,\n",
    "        key=lambda col: col if col.name not in orders else col.astype(orders[col.name]),\n",
    "    )\n",
    "    pivoted_sorted = pivoted_sorted.set_index(pivot_index)\n",
    "\n",
    "    return pivoted_sorted\n",
    "\n",
    "\n",
    "def generate_latex_table(df, latex_args, significance_level, green, formatter):\n",
    "    return (\n",
    "        df.style.map(lambda p: (green if float(p) < significance_level else \"\"))\n",
    "        .format(formatter=formatter)\n",
    "        .to_latex(**latex_args)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scientific_formatter(x):\n",
    "    if pd.notnull(x):\n",
    "        pretty = f\"{x:.2e}\"\n",
    "        return f\"\\\\num{{{pretty}}}\"\n",
    "    return f\"\\\\text{{NaN}}\"\n",
    "\n",
    "\n",
    "# Define constants\n",
    "columns = [\"Metric\", \"Submetric\", \"Edit Style\", \"System\", \"$p$-value\", \"Hypothesis\"]\n",
    "\n",
    "\n",
    "# Prepare the test results DataFrame\n",
    "renamed = prepare_test_results(test_results_df, columns)\n",
    "\n",
    "# Generate the LaTeX table\n",
    "green = \"background-color: kth-lightgreen\"\n",
    "latex = generate_latex_table(\n",
    "    renamed, latex_args, significance_level, green, scientific_formatter\n",
    ")\n",
    "\n",
    "# Save the LaTeX table to a file\n",
    "file_name = \"test_results.tex\"\n",
    "save_latex_table(latex, file_name)\n",
    "\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9675f9ae",
   "metadata": {},
   "source": [
    "### Compare Viking-Based Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "viking_results = []\n",
    "for metric in metrics:\n",
    "    major, minor = metric\n",
    "    for style in styles:\n",
    "        args = [(v, style, *metric) for v in vikings]\n",
    "        scores = [\n",
    "            grouped.get_group(arg).sort_values(by=\"Essay ID\")[\"Score\"].to_numpy()\n",
    "            for arg in args\n",
    "        ]\n",
    "        \"\"\"Explanation of the hypotheses:\n",
    "        3: Viking-7B > Viking-13B\n",
    "        4: Viking-7B < Viking-13B\n",
    "        \"\"\"\n",
    "        hypotheses = [3, 4]\n",
    "        for hypothesis in hypotheses:\n",
    "            stat, p_value = perform_statistical_test(\n",
    "                metric, scores, get_alternate_hypothesis(hypothesis)\n",
    "            )\n",
    "            viking_results.append(\n",
    "                {\n",
    "                    \"Edit Style\": style,\n",
    "                    \"Metric\": major,\n",
    "                    \"Submetric\": minor,\n",
    "                    \"statistic\": stat,\n",
    "                    \"$p$-value\": p_value,\n",
    "                    \"Hypothesis\": format_hypothesis(hypothesis),\n",
    "                }\n",
    "            )\n",
    "\n",
    "viking_results_df = pd.DataFrame(viking_results)\n",
    "# Prepare the test results DataFrame\n",
    "columns = [\"Metric\", \"Submetric\", \"Edit Style\", \"$p$-value\", \"Hypothesis\"]\n",
    "renamed = prepare_test_results(viking_results_df, columns)\n",
    "display(renamed)\n",
    "\n",
    "# Generate the LaTeX table\n",
    "green = \"background-color: kth-lightgreen\"\n",
    "latex = generate_latex_table(\n",
    "    renamed, latex_args, significance_level, green, scientific_formatter\n",
    ")\n",
    "\n",
    "# Save the LaTeX table to a file\n",
    "file_name = \"viking_pairwise_test_results.tex\"\n",
    "save_latex_table(latex, file_name)\n",
    "\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_results = []\n",
    "\n",
    "for metric in metrics:\n",
    "    major, minor = metric\n",
    "    for v in vikings:\n",
    "        args = [(v, style, *metric) for style in styles]\n",
    "        scores = [\n",
    "            grouped.get_group(arg).sort_values(by=\"Essay ID\")[\"Score\"].to_numpy()\n",
    "            for arg in args\n",
    "        ]\n",
    "        \"\"\"Explanation of the hypotheses:\n",
    "        5: Minimal > Fluency\n",
    "        6: Minimal < Fluency\n",
    "        \"\"\"\n",
    "        hypotheses = [5, 6]\n",
    "        for hypothesis in hypotheses:\n",
    "            stat, p_value = perform_statistical_test(\n",
    "                metric, scores, get_alternate_hypothesis(hypothesis)\n",
    "            )\n",
    "            style_results.append(\n",
    "                {\n",
    "                    \"System\": v,\n",
    "                    \"Metric\": major,\n",
    "                    \"Submetric\": minor,\n",
    "                    \"$p$-value\": p_value,\n",
    "                    \"Hypothesis\": format_hypothesis(hypothesis),\n",
    "                }\n",
    "            )\n",
    "\n",
    "style_results_df = pd.DataFrame(style_results)\n",
    "# Prepare the test results DataFrame\n",
    "columns = [\"Metric\", \"Submetric\", \"System\", \"$p$-value\", \"Hypothesis\"]\n",
    "renamed = prepare_test_results(style_results_df, columns)\n",
    "display(renamed)\n",
    "\n",
    "# Generate the LaTeX table\n",
    "green = \"background-color: kth-lightgreen\"\n",
    "latex = generate_latex_table(\n",
    "    renamed, latex_args, significance_level, green, scientific_formatter\n",
    ")\n",
    "\n",
    "# Save the LaTeX table to a file\n",
    "file_name = \"viking_style_test_results.tex\"\n",
    "save_latex_table(latex, file_name)\n",
    "\n",
    "print(latex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
