{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd682a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66362bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_csv_file = \"scores.csv\"\n",
    "auto_df = pd.read_csv(scores_csv_file)\n",
    "\n",
    "\n",
    "auto_df_long = auto_df.melt(\n",
    "    id_vars=[\"essay_id\", \"correction_style\", \"system\"],\n",
    "    value_vars=[\"gleu\", \"precision\", \"recall\", \"f0.5\", \"scribendi_score\"],\n",
    "    var_name=\"metric\",\n",
    "    value_name=\"score\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94909ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_evaluation_dir = \"manual_evaluation/\"\n",
    "d_key = \"evaluations\"\n",
    "\n",
    "manual_eval_dicts = []\n",
    "\n",
    "styles = auto_df_long[\"correction_style\"].unique().tolist()\n",
    "teams = auto_df_long[\"system\"].unique().tolist()\n",
    "\n",
    "for team in teams:\n",
    "    team_dir = path.join(manual_evaluation_dir, team)\n",
    "    for style in styles:\n",
    "        style_file_name = f\"{style}.json\"\n",
    "        style_file_path = path.join(team_dir, style_file_name)\n",
    "        with open(style_file_path) as f:\n",
    "            metric_df = json.load(f)\n",
    "        scores = metric_df[d_key]\n",
    "        for d in scores:\n",
    "            total = 0\n",
    "            for metric in [\"grammaticality\", \"fluency\", \"meaning_preservation\"]:\n",
    "                manual_eval_dicts.append(\n",
    "                    {\n",
    "                        \"essay_id\": d[\"id\"],\n",
    "                        \"correction_style\": style,\n",
    "                        \"system\": team,\n",
    "                        \"metric\": metric,\n",
    "                        \"score\": d[metric],\n",
    "                    }\n",
    "                )\n",
    "                total += d[metric]\n",
    "            manual_eval_dicts.append(\n",
    "                {\n",
    "                    \"essay_id\": d[\"id\"],\n",
    "                    \"correction_style\": style,\n",
    "                    \"system\": team,\n",
    "                    \"metric\": \"manual_evaluation\",\n",
    "                    \"score\": total / 3,\n",
    "                }\n",
    "            )\n",
    "\n",
    "manual_df_long = pd.DataFrame(manual_eval_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c3f801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      essay_id correction_style      system                metric      score\n",
      "0            1          minimal     UAM-CSI                  gleu  37.000000\n",
      "1            2          minimal     UAM-CSI                  gleu  49.210000\n",
      "2            3          minimal     UAM-CSI                  gleu  28.940000\n",
      "3            4          minimal     UAM-CSI                  gleu  19.600000\n",
      "4            5          minimal     UAM-CSI                  gleu  62.700000\n",
      "...        ...              ...         ...                   ...        ...\n",
      "1195        49          fluency  Viking-13B     manual_evaluation   2.666667\n",
      "1196        50          fluency  Viking-13B        grammaticality   3.000000\n",
      "1197        50          fluency  Viking-13B               fluency   2.000000\n",
      "1198        50          fluency  Viking-13B  meaning_preservation   2.000000\n",
      "1199        50          fluency  Viking-13B     manual_evaluation   2.333333\n",
      "\n",
      "[2700 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "dfs = [auto_df_long, manual_df_long]\n",
    "df_long = pd.concat(dfs)\n",
    "essay_ids = df_long[\"essay_id\"].unique().tolist()\n",
    "\n",
    "essay_id_subs = {essay_id: i for i, essay_id in enumerate(essay_ids, 1)}\n",
    "\n",
    "df_long[\"essay_id\"] = df_long[\"essay_id\"].map(essay_id_subs)\n",
    "\n",
    "print(df_long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff474140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Essay ID Edit Style      System                Metric      Score\n",
      "0            1    minimal     UAM-CSI                  gleu  37.000000\n",
      "1            2    minimal     UAM-CSI                  gleu  49.210000\n",
      "2            3    minimal     UAM-CSI                  gleu  28.940000\n",
      "3            4    minimal     UAM-CSI                  gleu  19.600000\n",
      "4            5    minimal     UAM-CSI                  gleu  62.700000\n",
      "...        ...        ...         ...                   ...        ...\n",
      "1195        49    fluency  Viking-13B     manual_evaluation   2.666667\n",
      "1196        50    fluency  Viking-13B        grammaticality   3.000000\n",
      "1197        50    fluency  Viking-13B               fluency   2.000000\n",
      "1198        50    fluency  Viking-13B  meaning_preservation   2.000000\n",
      "1199        50    fluency  Viking-13B     manual_evaluation   2.333333\n",
      "\n",
      "[2700 rows x 5 columns]\n",
      "      Essay ID Edit Style      System                Metric      Score\n",
      "0            1    Minimal     UAM-CSI                  gleu  37.000000\n",
      "1            2    Minimal     UAM-CSI                  gleu  49.210000\n",
      "2            3    Minimal     UAM-CSI                  gleu  28.940000\n",
      "3            4    Minimal     UAM-CSI                  gleu  19.600000\n",
      "4            5    Minimal     UAM-CSI                  gleu  62.700000\n",
      "...        ...        ...         ...                   ...        ...\n",
      "1195        49    Fluency  Viking-13B     manual_evaluation   2.666667\n",
      "1196        50    Fluency  Viking-13B        grammaticality   3.000000\n",
      "1197        50    Fluency  Viking-13B               fluency   2.000000\n",
      "1198        50    Fluency  Viking-13B  meaning_preservation   2.000000\n",
      "1199        50    Fluency  Viking-13B     manual_evaluation   2.333333\n",
      "\n",
      "[2700 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "output_headers = {\n",
    "    \"essay_id\": \"Essay ID\",\n",
    "    \"correction_style\": \"Edit Style\",\n",
    "    \"system\": \"System\",\n",
    "    \"metric\": \"Metric\",\n",
    "    \"score\": \"Score\",\n",
    "}\n",
    "\n",
    "df_long.rename(columns=output_headers, inplace=True)\n",
    "output_edit_styles = {\n",
    "    \"minimal\": \"Minimal\",\n",
    "    \"fluency\": \"Fluency\",\n",
    "}\n",
    "print(df_long)\n",
    "df_long[\"Edit Style\"] = df_long[\"Edit Style\"].map(output_edit_styles)\n",
    "print(df_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc2795c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gleu' 'precision' 'recall' 'f0.5' 'scribendi_score' 'grammaticality'\n",
      " 'fluency' 'meaning_preservation' 'manual_evaluation']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2700 entries, 0 to 1199\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Essay ID    2700 non-null   int64  \n",
      " 1   Edit Style  2700 non-null   object \n",
      " 2   System      2700 non-null   object \n",
      " 3   Metric      2700 non-null   object \n",
      " 4   Submetric   2700 non-null   object \n",
      " 5   Score       2700 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 147.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_long[\"Metric\"].unique())\n",
    "output_metrics = {\n",
    "    \"gleu\": \"GLEU\",\n",
    "    \"precision\": \"ERRANT;Precision\",\n",
    "    \"recall\": \"ERRANT;Recall\",\n",
    "    \"f0.5\": r\"ERRANT;$\\text{F}_{0.5}$-Score\",\n",
    "    \"scribendi_score\": \"Scribendi Score\",\n",
    "    \"grammaticality\": \"SOME;Grammaticality\",\n",
    "    \"fluency\": \"SOME;Fluency\",\n",
    "    \"meaning_preservation\": \"SOME;Meaning Preservation\",\n",
    "    \"manual_evaluation\": \"SOME;Total\",\n",
    "}\n",
    "df_long[\"Metric\"] = df_long[\"Metric\"].map(output_metrics)\n",
    "df_long[[\"Metric\", \"Submetric\"]] = df_long[\"Metric\"].str.split(\";\", expand=True)\n",
    "df_long[\"Submetric\"] = df_long[\"Submetric\"].str.strip().fillna(\"\")\n",
    "df_long = df_long[\n",
    "    [\n",
    "        \"Essay ID\",\n",
    "        \"Edit Style\",\n",
    "        \"System\",\n",
    "        \"Metric\",\n",
    "        \"Submetric\",\n",
    "        \"Score\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(df_long.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0e7115",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"scores_long.csv\"\n",
    "df_long.to_csv(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
