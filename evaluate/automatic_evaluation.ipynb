{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Evaluation\n",
    "\n",
    "This notebook performs the automatic evaluation of the system outputs.\n",
    "\n",
    "Make sure you have read the `README` in this directory and installed all required packages.\n",
    "\n",
    "## Imports\n",
    "\n",
    "Import all required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs, listdir, environ\n",
    "import subprocess\n",
    "import errant\n",
    "from scribendi import ScribendiScore\n",
    "from syntok.tokenizer import Tokenizer\n",
    "from itertools import product\n",
    "import spacy_udpipe\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Setup the repo root and m2-directory as constants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ROOT = path.join(\"/home/jovyan/da231x\")\n",
    "print(REPO_ROOT)\n",
    "M2DIR = path.join(REPO_ROOT, \"m2\")\n",
    "makedirs(M2DIR, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "FULL_CORPUS = \"full_corpus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processsing Functions\n",
    "\n",
    "Define functions to process the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path) as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def pretokenize(txt):\n",
    "    \"\"\"\n",
    "    Tokenizes and returns txt with syntok.tokenizer.\n",
    "    \"\"\"\n",
    "    tok = Tokenizer()\n",
    "    return \" \".join([str(token).strip() for token in tok.tokenize(txt)])\n",
    "\n",
    "\n",
    "def convert_essay_to_single_line(essay):\n",
    "    \"\"\"\n",
    "    Replace all newlines in essay with spaces.\n",
    "    \"\"\"\n",
    "    newline = \"\\n\"\n",
    "    space = \" \"\n",
    "    return essay.replace(newline, space)\n",
    "\n",
    "\n",
    "def md_to_dict(md):\n",
    "    \"\"\"\n",
    "    Parse shared task format into a dictionary where keys are essay IDs\n",
    "    and values are essay texts.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    md --- a string with the content of a shared task Markdown file.\n",
    "    \"\"\"\n",
    "    essay_dict = {}\n",
    "    for essay in md.split(\"### essay_id = \")[1:]:\n",
    "        (essay_id, text) = essay.split(\"\\n\", maxsplit=1)\n",
    "        text_tokenized = pretokenize(text).strip(\"\\n\")\n",
    "        essay_dict[essay_id] = convert_essay_to_single_line(text_tokenized)\n",
    "    return essay_dict\n",
    "\n",
    "\n",
    "def write_essay_to_file(output_dir, essay_id, essay_text):\n",
    "    \"\"\"\n",
    "    Writes essay text to the file path output_dir/essay_id.tmp and returns the file path.\n",
    "    \"\"\"\n",
    "    file_name = f\"{essay_id}.tmp\"\n",
    "    file_path = path.join(output_dir, file_name)\n",
    "    with open(file_path, \"w+\") as f:\n",
    "        f.write(essay_text)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def _ensure_directory_exists(directory):\n",
    "    \"\"\"\n",
    "    Creates directory if it does not exist.\n",
    "    \"\"\"\n",
    "    makedirs(directory, exist_ok=True)\n",
    "\n",
    "\n",
    "def split_file_per_essay(input_file, output_dir):\n",
    "    \"\"\"\n",
    "    Reads each essay from input_file and writes them to individual files.\n",
    "    The input file is structured as below:\n",
    "    ### essay_id = ABC123\n",
    "    ...\n",
    "    ### essay_id = XYZ987\n",
    "    ...\n",
    "\n",
    "    Each essay is written to a file with the path: output_dir/essay_id.tmp.\n",
    "\n",
    "    Returns a dict[essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    _ensure_directory_exists(output_dir)\n",
    "\n",
    "    input_text = read_file(input_file)\n",
    "\n",
    "    ids_texts = md_to_dict(input_text)\n",
    "\n",
    "    file_paths = {}\n",
    "\n",
    "    for essay_id, essay_text in ids_texts.items():\n",
    "        file_path = write_essay_to_file(output_dir, essay_id, essay_text)\n",
    "        file_paths[essay_id] = file_path\n",
    "\n",
    "    joined = \"\\n\".join(text for text in ids_texts.values())\n",
    "\n",
    "    full_corpus_file_path = path.join(output_dir, f\"{FULL_CORPUS}.tmp\")\n",
    "    with open(full_corpus_file_path, \"w+\") as f:\n",
    "        f.write(joined)\n",
    "    file_paths[FULL_CORPUS] = full_corpus_file_path\n",
    "\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "These variables are used to create file paths for later use in the evaluation process.\n",
    "\n",
    "### Edit Versions\n",
    "\n",
    "Setup variables to distinguish minimal edits and fluency edits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMAL = \"minimal\"\n",
    "FLUENCY = \"fluency\"\n",
    "versions = [MINIMAL, FLUENCY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = \"source\"\n",
    "HYPOTHESIS = \"hypothesis\"\n",
    "REFERENCE = \"reference\"\n",
    "\n",
    "PRECISION = \"precision\"\n",
    "RECALL = \"recall\"\n",
    "F05 = \"f05\"\n",
    "\n",
    "ESSAY_ID = \"essay id\"\n",
    "\n",
    "TEAM = \"team\"\n",
    "VERSION = \"version\"\n",
    "BATCH_SIZE = \"batch size\"\n",
    "\n",
    "SPACE = \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams\n",
    "\n",
    "Create a list of all teams, which are all directories under `../models/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = path.join(REPO_ROOT, \"outputs/\")\n",
    "\n",
    "teams = [\n",
    "    d for d in listdir(models_dir)\n",
    "    if path.isdir(path.join(models_dir, d))\n",
    "]\n",
    "print(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEAM_LABEL = \"Team\"\n",
    "STYLE_LABEL = \"Correction Style\"\n",
    "ESSAY_LABEL = \"Essay\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories\n",
    "\n",
    "Setup Directories for various directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = path.join(REPO_ROOT, \"data/swedish/SweLL-gold/\")\n",
    "SOURCE_DIR = path.join(REPO_ROOT, \"sources/\")\n",
    "REFERENCE_DIR = path.join(REPO_ROOT, \"references/\")\n",
    "HYPOTHESIS_DIR = path.join(REPO_ROOT, \"hypotheses/\")\n",
    "SYSTEM_OUTPUT_DIR = path.join(REPO_ROOT, \"outputs/\")\n",
    "\n",
    "print(DATA_DIR)\n",
    "print(SOURCE_DIR)\n",
    "print(REFERENCE_DIR)\n",
    "print(HYPOTHESIS_DIR)\n",
    "print(SYSTEM_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "Get paths for all source files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_source_paths():\n",
    "    \"\"\"\n",
    "    Writes all source essays to files on the form: `SOURCE_DIR/essay_id.tmp`.\n",
    "    Returns a dict[essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    md = path.join(DATA_DIR, \"sv-swell_gold-orig-test.md\")\n",
    "    return split_file_per_essay(md, SOURCE_DIR)\n",
    "\n",
    "\n",
    "source_paths = get_all_source_paths()\n",
    "print(source_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Get paths for all reference files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_paths(input_file, version):\n",
    "    \"\"\"\n",
    "    Writes all reference essays to files on the form: `REFERENCE_DIR/version/essay_id.tmp`.\n",
    "    Returns a dict[essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    output_dir = path.join(REFERENCE_DIR, version)\n",
    "    return split_file_per_essay(input_file, output_dir)\n",
    "\n",
    "\n",
    "def get_all_reference_paths():\n",
    "    \"\"\"\n",
    "    Writes both minimal-edited and fluency-edited reference essays to files on the form: `REFERENCE_DIR/version/essay_id.tmp`.\n",
    "    Returns a dict[version][essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    minimal_reference_md = path.join(DATA_DIR, \"sv-swell_gold-ref1-test.md\")\n",
    "    fluency_reference_md = path.join(DATA_DIR, \"sv-swell_gold-ref2-test.md\")\n",
    "    return {\n",
    "        MINIMAL: get_reference_paths(minimal_reference_md, MINIMAL),\n",
    "        FLUENCY: get_reference_paths(fluency_reference_md, FLUENCY),\n",
    "    }\n",
    "\n",
    "\n",
    "reference_paths = get_all_reference_paths()\n",
    "print(reference_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses\n",
    "\n",
    "Get paths for all hypothesis paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_version_hypothesis_paths(team, version, md):\n",
    "    \"\"\"\n",
    "    Writes all system hypotheses to files on the form: `HYPOTHESIS_DIR/team/version/essay_id.tmp`.\n",
    "    Returns a dict[essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    hypothesis_dir = path.join(HYPOTHESIS_DIR, team, version)\n",
    "    return split_file_per_essay(md, hypothesis_dir)\n",
    "\n",
    "\n",
    "def get_system_hypothesis_paths(team):\n",
    "    \"\"\"\n",
    "    Writes both minimal-edited and fluency-edited system hypotheses to files on the form: `HYPOTHESIS_DIR/team/version/essay_id.tmp`.\n",
    "    Returns a dict[version][essay_id] = file_path.\n",
    "    \"\"\"\n",
    "\n",
    "    minimal_hypothesis_md = path.join(\n",
    "        SYSTEM_OUTPUT_DIR, team, MINIMAL, \"sv-swell_gold-hypo-test.md\"\n",
    "    )\n",
    "    fluency_hypothesis_md = path.join(\n",
    "        SYSTEM_OUTPUT_DIR, team, FLUENCY, \"sv-swell_gold-fluency-hypo-test.md\"\n",
    "    )\n",
    "    return {\n",
    "        MINIMAL: get_system_version_hypothesis_paths(\n",
    "            team, MINIMAL, minimal_hypothesis_md\n",
    "        ),\n",
    "        FLUENCY: get_system_version_hypothesis_paths(\n",
    "            team, FLUENCY, fluency_hypothesis_md\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_all_hypothesis_paths():\n",
    "    \"\"\"\n",
    "    Writes both minimal-edited and fluency-edited system hypotheses for both teams to files on the form: `HYPOTHESIS_DIR/team/version/essay_id.tmp`.\n",
    "    Returns a dict[team][version][essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    return {team: get_system_hypothesis_paths(team) for team in teams}\n",
    "\n",
    "\n",
    "hypothesis_paths = get_all_hypothesis_paths()\n",
    "print(hypothesis_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_ids = list(source_paths.keys())\n",
    "essay_ids.remove(FULL_CORPUS)\n",
    "\n",
    "\n",
    "def compute_pairwise_scores(metric_function):\n",
    "    scores = {}\n",
    "    for team in tqdm(teams, desc=TEAM_LABEL):\n",
    "        scores[team] = {}\n",
    "        for version in tqdm(versions, leave=False, desc=STYLE_LABEL):\n",
    "            scores[team][version] = {}\n",
    "            for essay_id in tqdm(essay_ids, leave=False, desc=ESSAY_LABEL):\n",
    "                args = {\n",
    "                    SOURCE: source_paths[essay_id],\n",
    "                    MINIMAL: reference_paths[MINIMAL][essay_id],\n",
    "                    FLUENCY: reference_paths[FLUENCY][essay_id],\n",
    "                    HYPOTHESIS: hypothesis_paths[team][version][essay_id],\n",
    "                    TEAM: team,\n",
    "                    VERSION: version,\n",
    "                    ESSAY_ID: essay_id,\n",
    "                    BATCH_SIZE: 1,\n",
    "                }\n",
    "                scores[team][version][essay_id] = metric_function(args)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_corpus_scores(metric_function):\n",
    "    scores = {}\n",
    "    for team in tqdm(teams, desc=TEAM_LABEL):\n",
    "        scores[team] = {}\n",
    "        for version in tqdm(versions, leave=False, desc=STYLE_LABEL):\n",
    "            scores[team][version] = {}\n",
    "            args = {\n",
    "                SOURCE: source_paths[FULL_CORPUS],\n",
    "                SOURCE: source_paths[FULL_CORPUS],\n",
    "                MINIMAL: reference_paths[MINIMAL][FULL_CORPUS],\n",
    "                FLUENCY: reference_paths[FLUENCY][FULL_CORPUS],\n",
    "                HYPOTHESIS: hypothesis_paths[team][version][FULL_CORPUS],\n",
    "                TEAM: team,\n",
    "                VERSION: version,\n",
    "                ESSAY_ID: FULL_CORPUS,\n",
    "                BATCH_SIZE: 4,\n",
    "                FULL_CORPUS: True\n",
    "            }\n",
    "            scores[team][version][FULL_CORPUS] = metric_function(args)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Shell Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_env = environ.copy()\n",
    "\n",
    "\n",
    "def run_command(command_args):\n",
    "    command = SPACE.join(command_args)\n",
    "    result = subprocess.run(\n",
    "        command, shell=True, capture_output=True, env=current_env\n",
    "    )\n",
    "    # Verify that program exited correctly, otherwise raise exception\n",
    "    result.check_returncode()\n",
    "    output_bytes = result.stdout\n",
    "    output = output_bytes.decode(\"utf-8\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLEU\n",
    "\n",
    "Compute GLEU with the implementation by Shota Koyama ([https://github.com/shotakoyama/gleu](https://github.com/shotakoyama/gleu)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gleu(files):\n",
    "    gleu_command = [\n",
    "        f\"gleu\",\n",
    "        f\"-s {files[SOURCE]}\",\n",
    "        f\"-r {files[MINIMAL]} {files[FLUENCY]}\",  # Use both references\n",
    "        f\"-o {files[HYPOTHESIS]}\",\n",
    "        f\"-d 4\",  # Number of decimal places\n",
    "        f\"-f\",  # Fixed seed\n",
    "        f\"-n 4\",  # Maximum n-gram length\n",
    "        f\"-t word\",  # Word-level tokenization\n",
    "    ]\n",
    "\n",
    "    gleu_output = run_command(gleu_command)\n",
    "\n",
    "    if gleu_output != \"\":\n",
    "        gleu_split = gleu_output.split()\n",
    "        gleu_score = float(gleu_split[1])\n",
    "    else:\n",
    "        gleu_score = -float(\"inf\")\n",
    "    return gleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_gleu_scores = compute_pairwise_scores(compute_gleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_gleu_scores = compute_corpus_scores(compute_gleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_gleu_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERRANT\n",
    "\n",
    "Compute ERRANT with the implementation by Andrew Caines ([https://github.com/cainesap/errant](https://github.com/cainesap/errant)).\n",
    "\n",
    "Begin by defining some helper functions.\n",
    "\n",
    "Read a file and return its contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"sv\"\n",
    "\n",
    "\n",
    "def create_m2_file(original, corrected, output):\n",
    "    errant_parallel_command = [\n",
    "        f\"errant_parallel\",\n",
    "        f\"-orig {original}\",\n",
    "        f\"-cor {SPACE.join(corrected)}\",  # Use all provided references\n",
    "        f\"-out {output}\",\n",
    "        f\"-lang {language}\",\n",
    "    ]\n",
    "    # Capture output to dummy variable to avoid printing\n",
    "    dummy = run_command(errant_parallel_command)\n",
    "\n",
    "\n",
    "def prepare_m2_files(args):\n",
    "    # Create source-reference m2\n",
    "    reference_m2_dir = path.join(M2DIR, REFERENCE)\n",
    "    _ensure_directory_exists(reference_m2_dir)\n",
    "    reference_m2_path = path.join(reference_m2_dir, f\"{args[ESSAY_ID]}.m2\")\n",
    "\n",
    "    # Reference m2 does not change\n",
    "    if not path.isfile(reference_m2_path):\n",
    "        create_m2_file(\n",
    "            args[SOURCE],\n",
    "            [args[MINIMAL], args[FLUENCY]],\n",
    "            reference_m2_path\n",
    "        )\n",
    "\n",
    "    # Create source-hypothesis m2\n",
    "    hypothesis_m2_dir = path.join(M2DIR, args[TEAM], args[VERSION])\n",
    "    _ensure_directory_exists(hypothesis_m2_dir)\n",
    "    hypothesis_m2_path = path.join(hypothesis_m2_dir, f\"{args[ESSAY_ID]}.m2\")\n",
    "\n",
    "    create_m2_file(\n",
    "        args[SOURCE],\n",
    "        [args[HYPOTHESIS]],\n",
    "        hypothesis_m2_path\n",
    "    )\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = compute_pairwise_scores(prepare_m2_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = compute_corpus_scores(prepare_m2_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERRANT_REGEX = re.compile(r\"\\d\\.\\d+\\s+\\d\\.\\d+\\s+\\d\\.\\d+\")\n",
    "language = \"sv\"\n",
    "\n",
    "\n",
    "def compute_errant(args):\n",
    "    reference_m2_path = path.join(M2DIR, REFERENCE, f\"{args[ESSAY_ID]}.m2\")\n",
    "\n",
    "    hypothesis_m2_path = path.join(\n",
    "        M2DIR, args[TEAM], args[VERSION], f\"{args[ESSAY_ID]}.m2\"\n",
    "    )\n",
    "\n",
    "    errant_compare_command = [\n",
    "        f\"errant_compare\",\n",
    "        f\"-hyp {hypothesis_m2_path}\",\n",
    "        f\"-ref {reference_m2_path}\",\n",
    "    ]\n",
    "    scores = run_command(errant_compare_command)\n",
    "\n",
    "    # Capture the output, which looks like this. Then extract precision, recall and F0.5\n",
    "    # =========== Span-Based Correction ============\n",
    "    # TP      FP      FN      Prec    Rec     F0.5\n",
    "    # 12      4       6       0.75    0.6667  0.7317\n",
    "    # ==============================================\n",
    "\n",
    "    if scores != \"\":\n",
    "        regex_match = ERRANT_REGEX.search(scores)\n",
    "        match_list = regex_match.group(0).split(\"\\t\")\n",
    "        match_values = [100 * float(v) for v in match_list]\n",
    "\n",
    "        precision = match_values[0]\n",
    "        recall = match_values[1]\n",
    "        f05 = match_values[2]\n",
    "    else:\n",
    "        precision = -float(\"inf\")\n",
    "        recall = -float(\"inf\")\n",
    "        f05 = -float(\"inf\")\n",
    "\n",
    "    return {\n",
    "        PRECISION: precision,\n",
    "        RECALL: recall,\n",
    "        F05: f05,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_errant_scores = compute_pairwise_scores(compute_errant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_errant_scores = compute_corpus_scores(compute_errant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_errant_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scribendi Score\n",
    "\n",
    "Compute the Scribendi Score with the implementation by Robert Östling ([https://github.com/robertostling/scribendi_score](https://github.com/robertostling/scribendi_score)).\n",
    "\n",
    "Begin by setting up the model. Use the same `Llama-3.1-8B` model ([Hugging Face](https://huggingface.co/meta-llama/Llama-3.1-8B)) as in the shared task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scribendi_model = \"meta-llama/Llama-3.1-8B\"\n",
    "scribendi_access_token = \"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\"\n",
    "scribendi_scorer = ScribendiScore(\n",
    "    model_id=scribendi_model, access_token=scribendi_access_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scribendi_score(args: dict):\n",
    "    contents = {\n",
    "        SOURCE: read_file(args[SOURCE]),\n",
    "        HYPOTHESIS: read_file(args[HYPOTHESIS])\n",
    "    }\n",
    "\n",
    "    if args.get(FULL_CORPUS, False):\n",
    "        inputs = {\n",
    "            k: {\n",
    "                str(i): [text]\n",
    "                for i, text in enumerate(contents[k].splitlines())\n",
    "            }\n",
    "            for k in contents.keys()\n",
    "        }\n",
    "    else:\n",
    "        dummy_key = \"DUMMY\"\n",
    "        inputs = {\n",
    "            k: {\n",
    "                dummy_key: [contents[k]]\n",
    "            }\n",
    "            for k in contents.keys()\n",
    "        }\n",
    "\n",
    "    return scribendi_scorer.score(inputs[SOURCE], inputs[HYPOTHESIS], batch_size=args[BATCH_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_scribendi_scores = compute_pairwise_scores(compute_scribendi_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_scribendi_scores = compute_corpus_scores(compute_scribendi_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_scribendi_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the Scribendi-Score object to save VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del scribendi_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Scores\n",
    "\n",
    "Combine all scores into a single dataframe and save the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairwise_scores = []\n",
    "\n",
    "for team, version, essay_id in product(teams, versions, essay_ids):\n",
    "    gleu = pairwise_gleu_scores[team][version][essay_id]\n",
    "    precision = pairwise_errant_scores[team][version][essay_id][\"precision\"]\n",
    "    recall = pairwise_errant_scores[team][version][essay_id][\"recall\"]\n",
    "    f05 = pairwise_errant_scores[team][version][essay_id][\"f05\"]\n",
    "    scribendi_score = pairwise_scribendi_scores[team][version][essay_id]\n",
    "    all_pairwise_scores.append(\n",
    "        {\n",
    "            \"essay_id\": essay_id,\n",
    "            \"correction_style\": version,\n",
    "            \"system\": team,\n",
    "            \"gleu\": gleu,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f0.5\": f05,\n",
    "            \"scribendi_score\": scribendi_score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "pairwise_df = pd.DataFrame(all_pairwise_scores).round(2)\n",
    "pairwise_df.to_csv(\"scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corpus_scores = []\n",
    "\n",
    "for team, version in product(teams, versions):\n",
    "    gleu = corpus_gleu_scores[team][version][FULL_CORPUS]\n",
    "    precision = corpus_errant_scores[team][version][FULL_CORPUS][\"precision\"]\n",
    "    recall = corpus_errant_scores[team][version][FULL_CORPUS][\"recall\"]\n",
    "    f05 = corpus_errant_scores[team][version][FULL_CORPUS][\"f05\"]\n",
    "    scribendi_score = corpus_scribendi_scores[team][version][FULL_CORPUS]\n",
    "    all_corpus_scores.append(\n",
    "        {\n",
    "            \"correction_style\": version,\n",
    "            \"system\": team,\n",
    "            \"gleu\": gleu,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f0.5\": f05,\n",
    "            \"scribendi score\": scribendi_score / 50,\n",
    "        }\n",
    "    )\n",
    "\n",
    "corpus_df = pd.DataFrame(all_corpus_scores).round(2)\n",
    "corpus_df.to_csv(\"corpus_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multigec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
