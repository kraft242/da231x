{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Evaluation\n",
    "\n",
    "This notebook performs the automatic evaluation of the system outputs.\n",
    "\n",
    "Make sure you have read the `README` in this directory and installed all required packages.\n",
    "\n",
    "## Imports\n",
    "\n",
    "Import all required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs, popen, system, listdir\n",
    "import sys\n",
    "import errant\n",
    "from scribendi import ScribendiScore\n",
    "from syntok.tokenizer import Tokenizer\n",
    "from itertools import product\n",
    "import spacy_udpipe\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Setup the repo root and m2-directory as constants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ROOT = path.join(\"/home/jovyan/da231x\")\n",
    "print(REPO_ROOT)\n",
    "M2DIR = path.join(REPO_ROOT, \"m2\")\n",
    "makedirs(M2DIR, exist_ok=True)  # Ensure directory exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processsing Functions\n",
    "\n",
    "Define functions to process the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path) as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def pretokenize(txt):\n",
    "    \"\"\"\n",
    "    Tokenizes and returns txt with syntok.tokenizer.\n",
    "    \"\"\"\n",
    "    tok = Tokenizer()\n",
    "    return \" \".join([str(token).strip() for token in tok.tokenize(txt)])\n",
    "\n",
    "\n",
    "def convert_essay_to_single_line(essay):\n",
    "    \"\"\"\n",
    "    Replace all newlines in essay with spaces.\n",
    "    \"\"\"\n",
    "    newline = \"\\n\"\n",
    "    space = \" \"\n",
    "    return essay.replace(newline, space)\n",
    "\n",
    "\n",
    "def md_to_dict(md):\n",
    "    \"\"\"\n",
    "    Parse shared task format into a dictionary where keys are essay IDs\n",
    "    and values are essay texts.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    md --- a string with the content of a shared task Markdown file.\n",
    "    \"\"\"\n",
    "    essay_dict = {}\n",
    "    for essay in md.split(\"### essay_id = \")[1:]:\n",
    "        (essay_id, text) = essay.split(\"\\n\", maxsplit=1)\n",
    "        text_tokenized = pretokenize(text).strip(\"\\n\")\n",
    "        essay_dict[essay_id] = convert_essay_to_single_line(text_tokenized)\n",
    "    return essay_dict\n",
    "\n",
    "\n",
    "def write_essay_to_file(output_dir, essay_id, essay_text):\n",
    "    \"\"\"\n",
    "    Writes essay text to the file path output_dir/essay_id.tmp and returns the file path.\n",
    "    \"\"\"\n",
    "    file_name = f\"{essay_id}.tmp\"\n",
    "    file_path = path.join(output_dir, file_name)\n",
    "    with open(file_path, \"w+\") as f:\n",
    "        f.write(essay_text)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def _ensure_directory_exists(directory):\n",
    "    \"\"\"\n",
    "    Creates directory if it does not exist.\n",
    "    \"\"\"\n",
    "    makedirs(directory, exist_ok=True)\n",
    "\n",
    "\n",
    "def split_file_per_essay(input_file, output_dir):\n",
    "    \"\"\"\n",
    "    Reads each essay from input_file and writes them to individual files.\n",
    "    The input file is structured as below:\n",
    "    ### essay_id = ABC123\n",
    "    ...\n",
    "    ### essay_id = XYZ987\n",
    "    ...\n",
    "\n",
    "    Each essay is written to a file with the path: output_dir/essay_id.tmp.\n",
    "\n",
    "    Returns a dict[essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    _ensure_directory_exists(output_dir)\n",
    "\n",
    "    input_text = read_file(input_file)\n",
    "\n",
    "    ids_texts = md_to_dict(input_text)\n",
    "\n",
    "    file_paths = {}\n",
    "\n",
    "    for essay_id, essay_text in ids_texts.items():\n",
    "        file_path = write_essay_to_file(output_dir, essay_id, essay_text)\n",
    "        file_paths[essay_id] = file_path\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "These variables are used to create file paths for later use in the evaluation process.\n",
    "\n",
    "### Edit Versions\n",
    "\n",
    "Setup variables to distinguish minimal edits and fluency edits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMAL = \"minimal\"\n",
    "FLUENCY = \"fluency\"\n",
    "versions = [MINIMAL, FLUENCY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams\n",
    "\n",
    "Create a list of all teams, which are all directories under `../models/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = path.join(REPO_ROOT, \"outputs/\")\n",
    "\n",
    "teams = [\n",
    "    d for d in listdir(models_dir)\n",
    "    if path.isdir(path.join(models_dir, d))\n",
    "]\n",
    "print(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEAM_LABEL = \"Team\"\n",
    "STYLE_LABEL = \"Correction Style\"\n",
    "ESSAY_LABEL = \"Essay\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories\n",
    "\n",
    "Setup Directories for various directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = path.join(REPO_ROOT, \"data/swedish/SweLL-gold/\")\n",
    "SOURCE_DIR = path.join(REPO_ROOT, \"sources/\")\n",
    "REFERENCE_DIR = path.join(REPO_ROOT, \"references/\")\n",
    "HYPOTHESIS_DIR = path.join(REPO_ROOT, \"hypotheses/\")\n",
    "SYSTEM_OUTPUT_DIR = path.join(REPO_ROOT, \"outputs/\")\n",
    "\n",
    "print(DATA_DIR)\n",
    "print(SOURCE_DIR)\n",
    "print(REFERENCE_DIR)\n",
    "print(HYPOTHESIS_DIR)\n",
    "print(SYSTEM_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "Get paths for all source files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_source_paths():\n",
    "    \"\"\"\n",
    "    Writes all source essays to files on the form: `SOURCE_DIR/essay_id.tmp`.\n",
    "    Returns a dict[essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    md = path.join(DATA_DIR, \"sv-swell_gold-orig-test.md\")\n",
    "    return split_file_per_essay(md, SOURCE_DIR)\n",
    "\n",
    "\n",
    "source_paths = get_all_source_paths()\n",
    "print(source_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Get paths for all reference files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_paths(input_file, version):\n",
    "    \"\"\"\n",
    "    Writes all reference essays to files on the form: `REFERENCE_DIR/version/essay_id.tmp`.\n",
    "    Returns a dict[essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    output_dir = path.join(REFERENCE_DIR, version)\n",
    "    return split_file_per_essay(input_file, output_dir)\n",
    "\n",
    "\n",
    "def get_all_reference_paths():\n",
    "    \"\"\"\n",
    "    Writes both minimal-edited and fluency-edited reference essays to files on the form: `REFERENCE_DIR/version/essay_id.tmp`.\n",
    "    Returns a dict[version][essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    minimal_reference_md = path.join(DATA_DIR, \"sv-swell_gold-ref1-test.md\")\n",
    "    fluency_reference_md = path.join(DATA_DIR, \"sv-swell_gold-ref2-test.md\")\n",
    "    return {\n",
    "        MINIMAL: get_reference_paths(minimal_reference_md, MINIMAL),\n",
    "        FLUENCY: get_reference_paths(fluency_reference_md, FLUENCY),\n",
    "    }\n",
    "\n",
    "\n",
    "reference_paths = get_all_reference_paths()\n",
    "print(reference_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses\n",
    "\n",
    "Get paths for all hypothesis paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_version_hypothesis_paths(team, version, md):\n",
    "    \"\"\"\n",
    "    Writes all system hypotheses to files on the form: `HYPOTHESIS_DIR/team/version/essay_id.tmp`.\n",
    "    Returns a dict[essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    hypothesis_dir = path.join(HYPOTHESIS_DIR, team, version)\n",
    "    return split_file_per_essay(md, hypothesis_dir)\n",
    "\n",
    "\n",
    "def get_system_hypothesis_paths(team):\n",
    "    \"\"\"\n",
    "    Writes both minimal-edited and fluency-edited system hypotheses to files on the form: `HYPOTHESIS_DIR/team/version/essay_id.tmp`.\n",
    "    Returns a dict[version][essay_id] = file_path.\n",
    "    \"\"\"\n",
    "\n",
    "    minimal_hypothesis_md = path.join(\n",
    "        SYSTEM_OUTPUT_DIR, team, MINIMAL, \"sv-swell_gold-hypo-test.md\"\n",
    "    )\n",
    "    fluency_hypothesis_md = path.join(\n",
    "        SYSTEM_OUTPUT_DIR, team, FLUENCY, \"sv-swell_gold-fluency-hypo-test.md\"\n",
    "    )\n",
    "    return {\n",
    "        MINIMAL: get_system_version_hypothesis_paths(\n",
    "            team, MINIMAL, minimal_hypothesis_md\n",
    "        ),\n",
    "        FLUENCY: get_system_version_hypothesis_paths(\n",
    "            team, FLUENCY, fluency_hypothesis_md\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_all_hypothesis_paths():\n",
    "    \"\"\"\n",
    "    Writes both minimal-edited and fluency-edited system hypotheses for both teams to files on the form: `HYPOTHESIS_DIR/team/version/essay_id.tmp`.\n",
    "    Returns a dict[team][version][essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    return {team: get_system_hypothesis_paths(team) for team in teams}\n",
    "\n",
    "\n",
    "hypothesis_paths = get_all_hypothesis_paths()\n",
    "print(hypothesis_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLEU\n",
    "\n",
    "Compute GLEU with the implementation by Shota Koyama ([https://github.com/shotakoyama/gleu](https://github.com/shotakoyama/gleu)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gleu(\n",
    "    source_file, minimal_reference_file, fluency_reference_file, hypothesis_file\n",
    "):\n",
    "    gleu_command = [\n",
    "        f\"gleu\",\n",
    "        f\"-s {source_file}\",\n",
    "        # Use both references\n",
    "        f\"-r {minimal_reference_file} {fluency_reference_file}\",\n",
    "        f\"-o {hypothesis_file}\",\n",
    "        f\"-d 4\",  # Number of decimal places\n",
    "        f\"-f\",  # Fixed seed\n",
    "        f\"-n 4\",  # Maximum n-gram length\n",
    "        f\"-t word\",  # Word-level tokenization\n",
    "    ]\n",
    "\n",
    "    gleu_output = popen(\" \".join(gleu_command)).read()\n",
    "    if gleu_output != \"\":\n",
    "        gleu_split = gleu_output.split()\n",
    "        gleu_score = float(gleu_split[1])\n",
    "    else:\n",
    "        gleu_score = -float(\"inf\")\n",
    "    return gleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gleu_scores = {}\n",
    "essay_ids = set(source_paths.keys())\n",
    "for team in tqdm(teams, desc=TEAM_LABEL):\n",
    "    gleu_scores[team] = {}\n",
    "    for version in tqdm(versions, leave=False, desc=STYLE_LABEL):\n",
    "        gleu_scores[team][version] = {}\n",
    "        for essay_id in tqdm(essay_ids, leave=False, desc=ESSAY_LABEL):\n",
    "            source_file = source_paths[essay_id]\n",
    "            minimal_reference_file = reference_paths[MINIMAL][essay_id]\n",
    "            fluency_reference_file = reference_paths[FLUENCY][essay_id]\n",
    "            hypothesis_file = hypothesis_paths[team][version][essay_id]\n",
    "\n",
    "            # Compute GLEU score\n",
    "            gleu_score = compute_gleu(\n",
    "                source_file,\n",
    "                minimal_reference_file,\n",
    "                fluency_reference_file,\n",
    "                hypothesis_file,\n",
    "            )\n",
    "\n",
    "            gleu_scores[team][version][essay_id] = gleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gleu_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERRANT\n",
    "\n",
    "Compute ERRANT with the implementation by Andrew Caines ([https://github.com/cainesap/errant](https://github.com/cainesap/errant)).\n",
    "\n",
    "Begin by defining some helper functions.\n",
    "\n",
    "Read a file and return its contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path) as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute precision $ P $, recall $ R $, and $ F_{ \\beta } $-score.\n",
    "\n",
    "| Metric         | Formula                                                                                |\n",
    "|----------------|----------------------------------------------------------------------------------------|\n",
    "| $ P $          | $ \\frac{ TP }{ TP + FP } $                                                             |\n",
    "| $ R $          | $ \\frac{ TP }{ TP + FN } $                                                             |\n",
    "| $ F_{ \\beta} $ | $ \\frac{ ( 1 + \\beta^{ 2 } ) \\times ( P \\times R ) }{ ( \\beta^{ 2 } \\times P ) + R } $ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(tp, fp):\n",
    "    if tp + fp == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "\n",
    "def compute_recall(tp, fn):\n",
    "    if tp + fn == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "\n",
    "def compute_f_beta(precision, recall, beta=0.5):\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"sv\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.to_disk(f\"spacy_models/en_core_web_sm/\")\n",
    "spacy_udpipe.download(LANGUAGE)\n",
    "annotator = errant.load(LANGUAGE, nlp=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_to_tuple(edit):\n",
    "    return (\n",
    "        edit.o_start,\n",
    "        edit.o_end,\n",
    "        edit.o_str,\n",
    "        edit.c_start,\n",
    "        edit.c_end,\n",
    "        edit.c_str,\n",
    "        edit.type,\n",
    "    )\n",
    "\n",
    "\n",
    "def edits_to_set(edits):\n",
    "    return set(edit_to_tuple(e) for e in edits)\n",
    "\n",
    "\n",
    "def errant_parse_file(file_path):\n",
    "    text = read_file(file_path)\n",
    "    return annotator.parse(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errant(\n",
    "    source_file,\n",
    "    minimal_reference_file,\n",
    "    fluency_reference_file,\n",
    "    hypothesis_file,\n",
    "):\n",
    "    parsed = {\n",
    "        \"source\": errant_parse_file(source_file),\n",
    "        \"minimal\": errant_parse_file(minimal_reference_file),\n",
    "        \"fluency\": errant_parse_file(fluency_reference_file),\n",
    "        \"hypothesis\": errant_parse_file(hypothesis_file),\n",
    "    }\n",
    "\n",
    "    edits = {\n",
    "        \"minimal\": annotator.annotate(parsed[\"source\"], parsed[\"minimal\"]),\n",
    "        \"fluency\": annotator.annotate(parsed[\"source\"], parsed[\"fluency\"]),\n",
    "        \"hypothesis\": annotator.annotate(parsed[\"source\"], parsed[\"hypothesis\"]),\n",
    "    }\n",
    "\n",
    "    reference_m2 = set.union(\n",
    "        edits_to_set(edits[\"minimal\"]),\n",
    "        edits_to_set(edits[\"fluency\"])\n",
    "    )\n",
    "    hypothesis_m2 = edits_to_set(edits[\"hypothesis\"])\n",
    "\n",
    "    tp = len(reference_m2 & hypothesis_m2)\n",
    "    fp = len(hypothesis_m2 - reference_m2)\n",
    "    fn = len(reference_m2 - hypothesis_m2)\n",
    "\n",
    "    precision = compute_precision(tp, fp)\n",
    "    recall = compute_recall(tp, fn)\n",
    "    f05 = compute_f_beta(precision, recall, beta=0.5)\n",
    "    return precision, recall, f05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errant_scores = {}\n",
    "\n",
    "for team in tqdm(teams, desc=TEAM_LABEL):\n",
    "    errant_scores[team] = {}\n",
    "    for version in tqdm(versions, leave=False, desc=STYLE_LABEL):\n",
    "        errant_scores[team][version] = {}\n",
    "        for essay_id in tqdm(essay_ids, leave=False, desc=ESSAY_LABEL):\n",
    "            source_file = source_paths[essay_id]\n",
    "            minimal_reference_file = reference_paths[MINIMAL][essay_id]\n",
    "            fluency_reference_file = reference_paths[FLUENCY][essay_id]\n",
    "            hypothesis_file = hypothesis_paths[team][version][essay_id]\n",
    "\n",
    "            # Compute ERRANT score\n",
    "            precision, recall, f05 = compute_errant(\n",
    "                source_file,\n",
    "                minimal_reference_file,\n",
    "                fluency_reference_file,\n",
    "                hypothesis_file,\n",
    "            )\n",
    "\n",
    "            errant_scores[team][version][essay_id] = {\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f05\": f05,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(errant_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete errant-related objects to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nlp\n",
    "del annotator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scribendi Score\n",
    "\n",
    "Compute the Scribendi Score with the implementation by Robert Ã–stling ([https://github.com/robertostling/scribendi_score](https://github.com/robertostling/scribendi_score)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scribendi_model = \"meta-llama/Llama-3.1-8B\"\n",
    "scribendi_access_token = \"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\"\n",
    "scribendi_scorer = ScribendiScore(\n",
    "    model_id=scribendi_model, access_token=scribendi_access_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scribendi_score(source_file, hypothesis_file):\n",
    "    # Read source and hypothesis file contents\n",
    "    source_text = read_file(source_file)\n",
    "    hypothesis_text = read_file(hypothesis_file)\n",
    "\n",
    "    # The Scribendi-Score API requires dicts as input\n",
    "    dummy_id = \"1\"\n",
    "    source_input = {dummy_id: source_text}\n",
    "    hypothesis_input = {dummy_id: hypothesis_text}\n",
    "\n",
    "    return scribendi_scorer.score(source_input, hypothesis_input, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scribendi_scores = {}\n",
    "for team in tqdm(teams, desc=TEAM_LABEL):\n",
    "    scribendi_scores[team] = {}\n",
    "    for version in tqdm(versions, leave=False, desc=STYLE_LABEL):\n",
    "        scribendi_scores[team][version] = {}\n",
    "        for essay_id in tqdm(essay_ids, leave=False, desc=ESSAY_LABEL):\n",
    "            source_file = source_paths[essay_id]\n",
    "            hypothesis_file = hypothesis_paths[team][version][essay_id]\n",
    "\n",
    "            # Compute Scribendi score\n",
    "            scribendi_score = compute_scribendi_score(\n",
    "                source_file,\n",
    "                hypothesis_file,\n",
    "            )\n",
    "\n",
    "            scribendi_scores[team][version][essay_id] = scribendi_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the Scribendi-Score object to save VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del scribendi_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Scores\n",
    "\n",
    "Combine all scores into a single dataframe and save the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "\n",
    "for team, version, essay_id in product(teams, versions, essay_ids):\n",
    "    gleu = gleu_scores[team][version][essay_id]\n",
    "    precision = errant_scores[team][version][essay_id][\"precision\"]\n",
    "    recall = errant_scores[team][version][essay_id][\"recall\"]\n",
    "    f05 = errant_scores[team][version][essay_id][\"f05\"]\n",
    "    scribendi_score = scribendi_scores[team][version][essay_id]\n",
    "    all_scores.append(\n",
    "        {\n",
    "            \"Essay ID\": essay_id,\n",
    "            \"Correction Style\": version,\n",
    "            \"System\": team,\n",
    "            \"GLEU\": gleu,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F0.5\": f05,\n",
    "            \"Scribendi Score\": scribendi_score,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_scores)\n",
    "df.to_csv(\"scores.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multigec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
