{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Evaluation\n",
    "\n",
    "This notebook performs the automatic evaluation of the system outputs.\n",
    "\n",
    "Make sure you have read the `README` in this directory and installed all required packages.\n",
    "\n",
    "## Imports\n",
    "\n",
    "Import all required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs, popen, system, listdir\n",
    "import re\n",
    "import pandas as pd\n",
    "import tqdm.notebook as tqdm\n",
    "from itertools import product\n",
    "from syntok.tokenizer import Tokenizer\n",
    "from scribendi import ScribendiScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Setup the repo root and m2-directory as constants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ROOT = path.join(path.dirname(path.abspath(__file__)), \"..\")\n",
    "M2DIR = path.join(REPO_ROOT, \"m2\")\n",
    "makedirs(M2DIR, exist_ok=True)  # Ensure directory exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processsing Functions\n",
    "\n",
    "Define functions to process the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize(txt):\n",
    "    \"\"\"\n",
    "    Tokenizes and returns txt with syntok.tokenizer.\n",
    "    \"\"\"\n",
    "    tok = Tokenizer()\n",
    "    return \" \".join([str(token).strip() for token in tok.tokenize(txt)])\n",
    "\n",
    "\n",
    "def convert_essay_to_single_line(essay: str):\n",
    "    \"\"\"\n",
    "    Replace all newlines in essay with spaces.\n",
    "    \"\"\"\n",
    "    newline = \"\\n\"\n",
    "    space = \" \"\n",
    "    return essay.replace(newline, space)\n",
    "\n",
    "\n",
    "def md_to_dict(md):\n",
    "    \"\"\"\n",
    "    Parse shared task format into a dictionary where keys are essay IDs\n",
    "    and values are essay texts.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    md --- a string with the content of a shared task Markdown file.\n",
    "    \"\"\"\n",
    "    essay_dict = {}\n",
    "    for essay in md.split(\"### essay_id = \")[1:]:\n",
    "        (essay_id, text) = essay.split(\"\\n\", maxsplit=1)\n",
    "        text_tokenized = pretokenize(text).strip(\"\\n\")\n",
    "        essay_dict[essay_id] = convert_essay_to_single_line(text_tokenized)\n",
    "    return essay_dict\n",
    "\n",
    "\n",
    "def write_essay_to_file(output_dir, essay_id, essay_text):\n",
    "    \"\"\"\n",
    "    Writes essay text to the file path output_dir/essay_id.tmp and returns the file path.\n",
    "    \"\"\"\n",
    "    file_name = f\"{essay_id}\".tmp\n",
    "    file_path = path.join(output_dir, file_name)\n",
    "    with open(file_path, \"w+\") as f:\n",
    "        f.write(essay_text)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def _ensure_directory_exists(directory):\n",
    "    \"\"\"\n",
    "    Creates directory if it does not exist.\n",
    "    \"\"\"\n",
    "    makedirs(directory, exist_ok=True)\n",
    "\n",
    "\n",
    "def split_file_per_essay(input_file, output_dir):\n",
    "    \"\"\"\n",
    "    Reads each essay from input_file and writes them to individual files.\n",
    "    The input file is structured as below:\n",
    "    ### essay_id = ABC123\n",
    "    ...\n",
    "    ### essay_id = XYZ987\n",
    "    ...\n",
    "\n",
    "    Each essay is written to a file with the path: output_dir/essay_id.tmp.\n",
    "\n",
    "    Returns a dict[essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    _ensure_directory_exists(output_dir)\n",
    "\n",
    "    ids_texts = md_to_dict(input_file)\n",
    "\n",
    "    file_paths = {}\n",
    "\n",
    "    for essay_id, essay_text in ids_texts.items():\n",
    "        file_path = write_essay_to_file(essay_id, essay_text)\n",
    "        file_paths[essay_id] = file_path\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "These variables are used to create file paths for later use in the evaluation process.\n",
    "\n",
    "### Edit Versions\n",
    "\n",
    "Setup variables to distinguish minimal edits and fluency edits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMAL = \"minimal\"\n",
    "FLUENCY = \"fluency\"\n",
    "versions = [MINIMAL, FLUENCY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams\n",
    "\n",
    "Create a list of all teams, which are all directories under `../models/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = path.join(REPO_ROOT, \"models/\")\n",
    "\n",
    "teams = [d for d in listdir(models_dir) if path.isdir(path.join(models_dir, d))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories\n",
    "\n",
    "Setup Directories for various directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = path.join(REPO_ROOT, \"data/swedish/SweLL_gold/\")\n",
    "SOURCE_DIR = path.join(REPO_ROOT, \"sources/\")\n",
    "REFERENCE_DIR = path.join(REPO_ROOT, \"references/\")\n",
    "HYPOTHESIS_DIR = path.join(REPO_ROOT, \"hypotheses/\")\n",
    "SYSTEM_OUTPUT_DIR = path.join(REPO_ROOT, \"outputs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "Get paths for all source files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_source_paths():\n",
    "    \"\"\"\n",
    "    Writes all source essays to files on the form: `SOURCE_DIR/essay_id.tmp`.\n",
    "    Returns a dict[essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    md = path.join(DATA_DIR, \"sv-swell_gold-orig-test.md\")\n",
    "    return split_file_per_essay(md, SOURCE_DIR)\n",
    "\n",
    "\n",
    "source_paths = get_all_source_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Get paths for all reference files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_paths(input_file, version):\n",
    "    \"\"\"\n",
    "    Writes all reference essays to files on the form: `REFERENCE_DIR/version/essay_id.tmp`.\n",
    "    Returns a dict[essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    output_dir = path.join(REFERENCE_DIR, version)\n",
    "    return split_file_per_essay(input_file, output_dir)\n",
    "\n",
    "\n",
    "def get_all_reference_paths():\n",
    "    \"\"\"\n",
    "    Writes both minimal-edited and fluency-edited reference essays to files on the form: `REFERENCE_DIR/version/essay_id.tmp`.\n",
    "    Returns a dict[version][essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    minimal_reference_md = path.join(DATA_DIR, \"sv-swell_gold-ref1-test.md\")\n",
    "    fluency_reference_md = path.join(DATA_DIR, \"sv-swell_gold-ref2-test.md\")\n",
    "    return {\n",
    "        MINIMAL: get_reference_paths(minimal_reference_md, MINIMAL),\n",
    "        FLUENCY: get_reference_paths(fluency_reference_md, FLUENCY),\n",
    "    }\n",
    "\n",
    "\n",
    "reference_paths = get_all_reference_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses\n",
    "\n",
    "Get paths for all hypothesis paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_version_hypothesis_paths(team, version, md):\n",
    "    \"\"\"\n",
    "    Writes all system hypotheses to files on the form: `HYPOTHESIS_DIR/team/version/essay_id.tmp`.\n",
    "    Returns a dict[essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    hypothesis_dir = path.join(HYPOTHESIS_DIR, team, version)\n",
    "    return split_file_per_essay(md, hypothesis_dir)\n",
    "\n",
    "\n",
    "def get_system_hypothesis_paths(team):\n",
    "    \"\"\"\n",
    "    Writes both minimal-edited and fluency-edited system hypotheses to files on the form: `HYPOTHESIS_DIR/team/version/essay_id.tmp`.\n",
    "    Returns a dict[version][essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    minimal_hypothesis_md = path.join(\n",
    "        SYSTEM_OUTPUT_DIR, team, \"sv-swell_gold-hypo-test.md\"\n",
    "    )\n",
    "    fluency_hypothesis_md = path.join(\n",
    "        SYSTEM_OUTPUT_DIR, team, \"sv-swell_gold-fluency-hypo-test.md\"\n",
    "    )\n",
    "    return {\n",
    "        MINIMAL: get_system_version_hypothesis_paths(\n",
    "            team, MINIMAL, minimal_hypothesis_md\n",
    "        ),\n",
    "        FLUENCY: get_system_version_hypothesis_paths(\n",
    "            team, FLUENCY, fluency_hypothesis_md\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_all_hypothesis_paths():\n",
    "    \"\"\"\n",
    "    Writes both minimal-edited and fluency-edited system hypotheses for both teams to files on the form: `HYPOTHESIS_DIR/team/version/essay_id.tmp`.\n",
    "    Returns a dict[team][version][essay_id] = file_path.\n",
    "    \"\"\"\n",
    "    return {team: get_system_hypothesis_paths(team) for team in teams}\n",
    "\n",
    "\n",
    "hypothesis_paths = get_all_hypothesis_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLEU\n",
    "\n",
    "Compute GLEU with the implementation by Shota Koyama ([https://github.com/shotakoyama/gleu](https://github.com/shotakoyama/gleu)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gleu(\n",
    "    source_file, minimal_reference_file, fluency_reference_file, hypothesis_file\n",
    "):\n",
    "    gleu_command = [\n",
    "        f\"gleu\",\n",
    "        f\"-s {source_file}\",\n",
    "        f\"-r {minimal_reference_file} {fluency_reference_file}\",  # Use both references\n",
    "        f\"-o {hypothesis_file}\",\n",
    "        f\"-d 4\",  # Number of decimal places\n",
    "        f\"-f\",  # Fixed seed\n",
    "        f\"-n 4\",  # Maximum n-gram length\n",
    "        f\"-t word\",  # Word-level tokenization\n",
    "    ]\n",
    "\n",
    "    gleu_output = popen(\" \".join(gleu_command)).read()\n",
    "    if gleu_output != \"\":\n",
    "        gleu_split = gleu_output.split()\n",
    "        gleu_score = float(gleu_split[1])\n",
    "    else:\n",
    "        gleu_score = -float(\"inf\")\n",
    "    return gleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERRANT\n",
    "\n",
    "Compute ERRANT with the implementation by Andrew Caines ([https://github.com/cainesap/errant](https://github.com/cainesap/errant)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for extracting precision, recall, and F0.5 from ERRANT output\n",
    "ERRANT_REGEX = re.compile(r\"\\d\\.\\d+\\s+\\d\\.\\d+\\s+\\d\\.\\d+\")\n",
    "\n",
    "\n",
    "def compute_errant(\n",
    "    source_file,\n",
    "    minimal_reference_file,\n",
    "    fluency_reference_file,\n",
    "    hypothesis_file,\n",
    "    essay_id,\n",
    "    version,\n",
    "    team,\n",
    "):\n",
    "    # ERRANT align source and reference files, from md to m2 (if needed)\n",
    "    reference_m2 = path.join(M2DIR, f\"{essay_id}-{version}-reference.m2\")\n",
    "    if not path.isfile(reference_m2):\n",
    "        errant_parallel_reference_command = [\n",
    "            f\"errant_parallel\",\n",
    "            f\"-orig {source_file}\",\n",
    "            f\"-cor {minimal_reference_file} {fluency_reference_file}\",  # Use both references\n",
    "            f\"-out {reference_m2}\",\n",
    "            f\"-lang SV\",  # Set language to Swedish\n",
    "        ]\n",
    "        system(\" \".join(errant_parallel_reference_command))\n",
    "\n",
    "    # ERRANT align source and hypothesis files, from md to m2 (if needed)\n",
    "    hypothesis_m2 = path.join(M2DIR, f\"{team}-{version}-{essay_id}.m2\")\n",
    "    if not path.isfile(hypothesis_m2):\n",
    "        errant_parallel_hypothesis_command = [\n",
    "            f\"errant_parallel\",\n",
    "            f\"-orig {source_file}\",\n",
    "            f\"-cor {hypothesis_file}\",\n",
    "            f\" -out {hypothesis_m2}\",\n",
    "            f\"-lang SV\",  # Set language to Swedish\n",
    "        ]\n",
    "        system(\" \".join(errant_parallel_hypothesis_command))\n",
    "\n",
    "    errant_compare_command = [\n",
    "        f\"errant_compare\",\n",
    "        f\"-hyp {hypothesis_m2}\",  # Aligned hypothesis file\n",
    "        f\"-ref {reference_m2}\",  # Aligned reference file\n",
    "    ]\n",
    "\n",
    "    errant_scores = popen(\" \".join(errant_compare_command)).read()\n",
    "\n",
    "    # capture the output which looks like this, add prec/rec/F0.5 to the output file\n",
    "    # =========== Span-Based Correction ============\n",
    "    # TP      FP      FN      Prec    Rec     F0.5\n",
    "    # 12      4       6       0.75    0.6667  0.7317\n",
    "    # ==============================================\n",
    "\n",
    "    if errant_scores != \"\":\n",
    "        prf_search = ERRANT_REGEX.search(errant_scores)\n",
    "        prf_list = prf_search.group(0).split(\"\\t\")\n",
    "        prf_values = [x for x in prf_list if x]\n",
    "        precision = prf_values[0]\n",
    "        recall = prf_values[1]\n",
    "        f05 = prf_values[2]\n",
    "        return float(precision), float(recall), float(f05)\n",
    "    else:\n",
    "        return tuple(-float(\"inf\")) * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scribendi Score\n",
    "\n",
    "Compute the Scribendi Score with the implementation by Robert Ã–stling ([https://github.com/robertostling/scribendi_score](https://github.com/robertostling/scribendi_score)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ScribendiScore's API to compute the Scribendi score\n",
    "scribendi_scorer = ScribendiScore()\n",
    "\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"\n",
    "    Helper function that reads a file and returns its content.\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def compute_scribendi_score(source_file, hypothesis_file):\n",
    "    # Read source and hypothesis file contents\n",
    "    source_text = read_file(source_file)\n",
    "    hypothesis_text = read_file(hypothesis_file)\n",
    "    \n",
    "    # The Scribendi-Score API requires dicts as input\n",
    "    dummy_id = \"1\"\n",
    "    source_input = {dummy_id: source_text}\n",
    "    hypothesis_input = {dummy_id: hypothesis_text}\n",
    "\n",
    "    return scribendi_scorer.score(source_input, hypothesis_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute All Scores\n",
    "\n",
    "Compute all scores and return a dict. The dicts are then used in a row in a pandas DataFrame that contains all scores.\n",
    "\n",
    "The final DataFrame is saved to `scores.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(\n",
    "    essay_id,\n",
    "    version,\n",
    "    team,\n",
    "    source_file,\n",
    "    minimal_reference_file,\n",
    "    fluency_reference_file,\n",
    "    hypothesis_file,\n",
    "):\n",
    "    # GLEU\n",
    "    gleu = compute_gleu(\n",
    "        source_file, minimal_reference_file, fluency_reference_file, hypothesis_file\n",
    "    )\n",
    "    # ERRANT\n",
    "    precision, recall, f05 = compute_errant(\n",
    "        source_file,\n",
    "        minimal_reference_file,\n",
    "        fluency_reference_file,\n",
    "        hypothesis_file,\n",
    "        essay_id,\n",
    "        version,\n",
    "        team,\n",
    "    )\n",
    "    # Scribendi Score\n",
    "    scribendi_score = compute_scribendi_score(source_file, hypothesis_file)\n",
    "\n",
    "    # Returned dict represents a row in the final CSV\n",
    "    return {\n",
    "        \"Essay ID\": essay_id,\n",
    "        \"Correction Style\": version,\n",
    "        \"System\": team,\n",
    "        \"GLEU\": gleu,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F0.5\": f05,\n",
    "        \"Scribendi Score\": scribendi_score,\n",
    "    }\n",
    "\n",
    "\n",
    "essay_ids = source_paths.keys()\n",
    "\n",
    "n_iterations = len(essay_ids) * len(versions) * len(teams)\n",
    "\n",
    "all_scores_list = []\n",
    "\n",
    "for essay_id, version, team in tqdm(\n",
    "    product(essay_ids, versions, teams), total=n_iterations\n",
    "):\n",
    "    # Retrieve file paths\n",
    "    source_file = source_paths[essay_id]\n",
    "    minimal_reference_file = reference_paths[MINIMAL][essay_id]\n",
    "    fluency_reference_file = reference_paths[FLUENCY][essay_id]\n",
    "    hypothesis_file = hypothesis_paths[team][version][essay_id]\n",
    "\n",
    "    # Compute scores\n",
    "    scores_dict = compute_scores(\n",
    "        essay_id,\n",
    "        version,\n",
    "        team,\n",
    "        source_file,\n",
    "        minimal_reference_file,\n",
    "        fluency_reference_file,\n",
    "        hypothesis_file,\n",
    "    )\n",
    "\n",
    "    # Save scores to list\n",
    "    all_scores_list.append(scores_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Save Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save a CSV file with all scores\n",
    "df = pd.DataFrame(all_scores_list)\n",
    "csv_file_name = \"scores.csv\"\n",
    "df.to_csv(csv_file_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
